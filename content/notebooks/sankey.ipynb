{
 "metadata": {
  "name": "",
  "signature": "sha256:eb5dc9c3e02e2df3ebdbbf1100ded69038fbe95f13387af04c586f6202d28760"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm currently working on the final project for my data visualization course. The dataset that I've chosen to work with can be [downloaded here](http://snap.stanford.edu/data/web-Reddit.html) -- it's a compendium Reddit resubmissions over a period of several years (ie, images that were submitted to more than once and/or to multiple subreddits). I waffled for a long time trying to decide what the best way to visualize the *flow* of images through various subreddits would be, but just in the nick of time, I stumbled across Christopher Gandrud's new [d3Network package for R](http://christophergandrud.github.io/d3Network/), and that was enough cause for me to settle on a Sankey diagram. If you've never heard of Reddit, the illustrious CPG Grey will enlighten you."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import YouTubeVideo\n",
      "YouTubeVideo('tlI022aUWQQ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "        <iframe\n",
        "            width=\"400\"\n",
        "            height=300\"\n",
        "            src=\"https://www.youtube.com/embed/tlI022aUWQQ\"\n",
        "            frameborder=\"0\"\n",
        "            allowfullscreen\n",
        "        ></iframe>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.lib.display.YouTubeVideo at 0x1098c1110>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The task of massaging columnar data consisting of an image ID, subreddit name, and timestamp for each submission into a more network-y format suitable for this type of visualization was interesting enough that I thought it might be a good post. If nothing else, Christopher's awesome package deserves some love.\n",
      "\n",
      "Python is my go-to language for data munging of this calibre, so we will use a Pandas -> NetworkX -> R -> D3 worflow. Without further ado, lets load the Python modules we will need and take a look at the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load modules and data\n",
      "\n",
      "import pandas as pd                        # For reading/munging the data\n",
      "import networkx as nx                      # For creating a graph structure\n",
      "from networkx.readwrite import json_graph  # For exporting a graph structure\n",
      "from itertools import islice               # For some more interesting munging\n",
      "\n",
      "%load_ext rmagic                           \n",
      "from IPython.display import HTML           # To display results when we're done\n",
      "\n",
      "!csvclean redditSubmissions.csv\n",
      "d = pd.read_csv('redditSubmissions_out.csv')\n",
      "d.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6 errors logged to redditSubmissions_err.csv\r\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>#image_id</th>\n",
        "      <th>unixtime</th>\n",
        "      <th>rawtime</th>\n",
        "      <th>title</th>\n",
        "      <th>total_votes</th>\n",
        "      <th>reddit_id</th>\n",
        "      <th>number_of_upvotes</th>\n",
        "      <th>subreddit</th>\n",
        "      <th>number_of_downvotes</th>\n",
        "      <th>localtime</th>\n",
        "      <th>score</th>\n",
        "      <th>number_of_comments</th>\n",
        "      <th>username</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1333172439</td>\n",
        "      <td> 2012-03-31T12:40:39.590113-07:00</td>\n",
        "      <td>          And here's a downvote.</td>\n",
        "      <td> 63470</td>\n",
        "      <td> rmqjs</td>\n",
        "      <td> 32657</td>\n",
        "      <td>    funny</td>\n",
        "      <td> 30813</td>\n",
        "      <td> 1333197639</td>\n",
        "      <td> 1844</td>\n",
        "      <td> 622</td>\n",
        "      <td> Animates_Everything</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1333178161</td>\n",
        "      <td> 2012-03-31T14:16:01.093638-07:00</td>\n",
        "      <td>                     Expectation</td>\n",
        "      <td>    35</td>\n",
        "      <td> rmun4</td>\n",
        "      <td>    29</td>\n",
        "      <td> GifSound</td>\n",
        "      <td>     6</td>\n",
        "      <td> 1333203361</td>\n",
        "      <td>   23</td>\n",
        "      <td>   3</td>\n",
        "      <td>       Gangsta_Raper</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1333199913</td>\n",
        "      <td> 2012-03-31T20:18:33.192906-07:00</td>\n",
        "      <td>                        Downvote</td>\n",
        "      <td>    41</td>\n",
        "      <td> rna86</td>\n",
        "      <td>    32</td>\n",
        "      <td> GifSound</td>\n",
        "      <td>     9</td>\n",
        "      <td> 1333225113</td>\n",
        "      <td>   23</td>\n",
        "      <td>   0</td>\n",
        "      <td>       Gangsta_Raper</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1333252330</td>\n",
        "      <td>        2012-04-01T10:52:10-07:00</td>\n",
        "      <td> Every time I downvote something</td>\n",
        "      <td>    10</td>\n",
        "      <td> ro7e4</td>\n",
        "      <td>     6</td>\n",
        "      <td> GifSound</td>\n",
        "      <td>     4</td>\n",
        "      <td> 1333277530</td>\n",
        "      <td>    2</td>\n",
        "      <td>   0</td>\n",
        "      <td>       Gangsta_Raper</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1333272954</td>\n",
        "      <td> 2012-04-01T16:35:54.393381-07:00</td>\n",
        "      <td>  Downvote &amp;quot;Dies Irae&amp;quot;</td>\n",
        "      <td>    65</td>\n",
        "      <td> rooof</td>\n",
        "      <td>    57</td>\n",
        "      <td> GifSound</td>\n",
        "      <td>     8</td>\n",
        "      <td> 1333298154</td>\n",
        "      <td>   49</td>\n",
        "      <td>   0</td>\n",
        "      <td>       Gangsta_Raper</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 13 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "   #image_id    unixtime                           rawtime  \\\n",
        "0          0  1333172439  2012-03-31T12:40:39.590113-07:00   \n",
        "1          0  1333178161  2012-03-31T14:16:01.093638-07:00   \n",
        "2          0  1333199913  2012-03-31T20:18:33.192906-07:00   \n",
        "3          0  1333252330         2012-04-01T10:52:10-07:00   \n",
        "4          0  1333272954  2012-04-01T16:35:54.393381-07:00   \n",
        "\n",
        "                             title  total_votes reddit_id  number_of_upvotes  \\\n",
        "0           And here's a downvote.        63470     rmqjs              32657   \n",
        "1                      Expectation           35     rmun4                 29   \n",
        "2                         Downvote           41     rna86                 32   \n",
        "3  Every time I downvote something           10     ro7e4                  6   \n",
        "4   Downvote &quot;Dies Irae&quot;           65     rooof                 57   \n",
        "\n",
        "  subreddit  number_of_downvotes   localtime  score  number_of_comments  \\\n",
        "0     funny                30813  1333197639   1844                 622   \n",
        "1  GifSound                    6  1333203361     23                   3   \n",
        "2  GifSound                    9  1333225113     23                   0   \n",
        "3  GifSound                    4  1333277530      2                   0   \n",
        "4  GifSound                    8  1333298154     49                   0   \n",
        "\n",
        "              username  \n",
        "0  Animates_Everything  \n",
        "1        Gangsta_Raper  \n",
        "2        Gangsta_Raper  \n",
        "3        Gangsta_Raper  \n",
        "4        Gangsta_Raper  \n",
        "\n",
        "[5 rows x 13 columns]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that I first called `csvclean`, from the [csvkit suite of command line utilities](http://csvkit.readthedocs.org/en/latest/index.html). The bang (!) symbol calls the command line from IPython. `csvclean` fixes a couple of formatting errors in the original dataset that interfere with R/Panda's parsing functions (something to do with quotes or commas in the \"title\" field, I believe). The repaired CSV is saved with a `_out` prepended to the filename. Nothing fancy is required for the `read_csv` call in our case.\n",
      "\n",
      "Now for the hard/interesting part. How to we map the flow of each image submission through the various subreddits?\n",
      " - First we sort by image and (crucially) timetamp on line 4.\n",
      " - On line 5, I simply extract the 3 columns that we care about. \n",
      " - Now we drop resubmissions of each image to the *same* subreddit with drop_duplicates on line 6, which only keeps each image's *first* submission to a particular subreddit (why we sorted first). \n",
      " - The last thing we need Pandas for is to group by image ID ID (line 7).\n",
      "\n",
      "On line 7, we pull the list of subreddits (now unique and nicely ordered) for each image. The nested list comprehension is necessary only because calling `.subreddit` on the groupby object `g` returns a tuple by default, and we'd rather have a list of lists."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Identify the order in which each image is submitted to various subreddits, \n",
      "# removing repeats within a subreddit\n",
      "\n",
      "g = d.sort(['#image_id', 'unixtime'])\\\n",
      "     .ix[:,['#image_id', 'unixtime', 'subreddit']]\\\n",
      "     .drop_duplicates(cols = ['#image_id', 'subreddit'])\\\n",
      "     .groupby('#image_id')\n",
      "flow = [[el for el in x[1]] for x in list(g.subreddit)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need a function `window` that rolls along each of the lists in `flow` and connects every subsequent pair of subreddits that a a particular image was submitted to. We'll do this with the help of the wonderful `itertools` module, creating two dimensional tuples that encode the \"from\" subreddit and the \"to\" subreddit, respectively. In lines 14 and 15, we apply the function and flatten the result to a single list.\n",
      "\n",
      "In order to truly capture the \"flow,\" however, we need to distinguish between the \"gifs\" subreddit node where images are popping up for the first time and the \"gifs\" subreddit node when the image has already appeared in another subreddit (say, \"pics\"). The `enumerate` in line 14 does this by tacking on the ordinality to the name of the node, admittedly very hacky, but we have a lot of tuples floating around already."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Roll along the list of subreddits each image has been submitted to, \n",
      "# creating an edge tuple for each subsequent pair\n",
      "def window(seq, n=2):\n",
      "    '''Returns a sliding window (of width n) over data from the iterable\n",
      "       s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...'''\n",
      "    it = iter(seq)\n",
      "    result = tuple(islice(it, n))\n",
      "    if len(result) == n:\n",
      "        yield result    \n",
      "    for elem in it:\n",
      "        result = result[1:] + (elem,)\n",
      "        yield result\n",
      "\n",
      "sankey = [list(window([str(i) + x for i, x in enumerate(sub)])) for sub in flow]\n",
      "sankey = [item for sublist in sankey for item in sublist] # flatten"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At last we have a list of edges that on some level describes the flow that we are trying to get at. Now we can just iterate through them and use `NetworkX` to create the graph and weight the edges appropriately. In lines 10--13, I prune back the tiny edges that clutter up the diagram, and then the nodes that are no longer associated with any edges. Last but not least, we export the structure to a JSON in line 16."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create network structure\n",
      "S = nx.DiGraph()\n",
      "for edge in sankey:\n",
      "    if S.has_edge(*edge):\n",
      "        S[edge[0]][edge[1]]['weight'] +=1\n",
      "    else:\n",
      "        S.add_edge(*edge, weight = 1)\n",
      "        \n",
      "# Trim edges\n",
      "S.remove_edges_from([x for x in S.edges(data=True) if x[2]['weight'] < 50])\n",
      "flagged = [x for x, el in S.out_degree().items() if (x[0] != '3') & (el == 0)]\n",
      "S.remove_edges_from([x for x in S.edges(data=True) if x[1] in flagged])\n",
      "S.remove_nodes_from([x for x, n in S.degree().items() if n == 0])\n",
      "\n",
      "# Export\n",
      "json_graph.dump(S, open('sankey.json', 'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Time for R!\n",
      "\n",
      "We will need to make sure that `d3Network` is installed to the the instance of R that is used by IPython's `Rmagic` via `Rpy2`. It was a different for me (I think?) so if you are running something like this for the first time, include the lines that are commented out.\n",
      "\n",
      "The `%%R` denotes block-level R magicks in IPython (`%R` will give you line-level magicks)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "#install.packages('devtools')\n",
      "#library(devtools)\n",
      "#devtools::install_github(\"christophergandrud/d3Network\")\n",
      "library(d3Network)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is finally the point at which Christopher Gandrud's package simplifies everything for us. We simply read in the nodes and linkes (edges) from the JSON file (they get converted to two dataframes). Note that we have to strip the janky ordinality numbers that we tacked onto the node names (line 3). Now that different nodes have the same names, the package will even make sure that each subreddit node has the same color every time it appears!\n",
      "\n",
      "The call to `d3Sankey` points to the the nodes dataframe, the links dataframe, the name of the sources/targets in the links dataframe, the name of the column that holds the link weights, and then some display configuration stuff."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "nodes <- JSONtoDF(file = paste0('sankey.json'), array = 'nodes')\n",
      "nodes$id <- substring(nodes$id, 2)\n",
      "links <- JSONtoDF(file = paste0('sankey.json'), array = 'links')\n",
      "d3Sankey(Nodes = nodes, Links = links, Source = 'source',\n",
      "         Target = 'target', Value = 'weight', NodeID = 'id', \n",
      "         width = 900, height = 600, fontsize = 12,\n",
      "         standAlone = TRUE, iframe = TRUE, file = '../extra/sankey.html')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<iframe src='../extra/sankey.html' height=642 width=927></iframe>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML('<iframe src=\"../extra/sankey.html\" height=620 width=1000 frameBorder=\"0\"></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe src=\"../extra/sankey.html\" height=620 width=1000 frameBorder=\"0\"></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "<IPython.core.display.HTML at 0x1098c1410>"
       ]
      }
     ],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}