<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>justmytwospence</title><link>http://pelican.spencerboucher.com/</link><description></description><atom:link href="http://pelican.spencerboucher.com/feeds/spencer-boucher.rss.xml" rel="self"></atom:link><lastBuildDate>Mon, 02 Jun 2014 08:51:10 -0700</lastBuildDate><item><title>Visualizing subreddit activity</title><link>http://pelican.spencerboucher.com/reddit/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;As a follow-up to the subreddit flow diagram posted a few weeks ago, here is the full Shiny app featuring a few other visualization techniques. The data set is large enough that it was causing problems for my shinyapps.io account, so I set up my own Shiny server on a free-tier EC2 instance. It still takes a hot minute to load at the time of writing, at least until I figure out how to optimize performance.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class="text-center"&gt;
&lt;a class="btn btn-primary btn-lg" href="http://shiny.spencerboucher.com/reddit" target="_blank" type="button"&gt;
      Explore subreddits    
    &lt;/a&gt;
&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Mon, 02 Jun 2014 08:51:10 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2014-06-02:reddit/</guid><category>ec2</category><category>shiny</category><category>visualization</category></item><item><title>Sankey diagram fun</title><link>http://pelican.spencerboucher.com/sankey/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;
&lt;/p&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I'm currently working on the final project for my data visualization course. The dataset that I've chosen to work with can be &lt;a href="http://snap.stanford.edu/data/web-Reddit.html"&gt;downloaded here&lt;/a&gt; -- it's a compendium Reddit resubmissions over a period of several years (ie, images that were submitted to more than one and/or to multiple subreddits). I waffled for a long time trying to decide what the best way to visualize the &lt;em&gt;flow&lt;/em&gt; of images through various subreddits would be, but just in the nick of time, I stumbled across Christopher Gandrud's new &lt;a href="http://christophergandrud.github.io/d3Network/"&gt;d3Network package for R&lt;/a&gt;, and that was enough cause for me to settle on a Sankey diagram. If you've never heard of Reddit, the illustrious CPG Grey will enlighten you.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
In [1]:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.display&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;YouTubeVideo&lt;/span&gt;
&lt;span class="n"&gt;YouTubeVideo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'tlI022aUWQQ'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;&lt;div class="prompt output_prompt"&gt;
    Out[1]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_pyout"&gt;
&lt;iframe allowfullscreen="" frameborder="0" height='300"' src="https://www.youtube.com/embed/tlI022aUWQQ" width="400"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The task of massaging columnar data consisting of an image ID, subreddit name, and timestamp for each submission into a more networky format suitable for this type of visualization was interesting enough that I thought it might be a good post. If nothing else, Christopher's awesome package deserves some love.&lt;/p&gt;
&lt;p&gt;Python is my go-to language for data munging of this calibre, so we will use a Pandas -&amp;gt; NetworkX -&amp;gt; R -&amp;gt; D3 worflow. Without further ado, lets load the Python modules we will need and take a look at the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
In [3]:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="c"&gt;# Load modules and data&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;                        &lt;span class="c"&gt;# For reading/munging the data&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;networkx&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nx&lt;/span&gt;                      &lt;span class="c"&gt;# For creating a graph structure&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;networkx.readwrite&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;json_graph&lt;/span&gt;  &lt;span class="c"&gt;# For exporting a graph structure&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;islice&lt;/span&gt;               &lt;span class="c"&gt;# For some more interesting munging&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;load_ext&lt;/span&gt; &lt;span class="n"&gt;rmagic&lt;/span&gt;                           
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.display&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;HTML&lt;/span&gt;           &lt;span class="c"&gt;# To display results when we're done&lt;/span&gt;

&lt;span class="o"&gt;!&lt;/span&gt;csvclean redditSubmissions.csv
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'redditSubmissions_out.csv'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;
Couldn't import dot_parser, loading of dot files will not be possible.
6 errors logged to redditSubmissions_err.csv

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;&lt;div class="prompt output_prompt"&gt;
    Out[3]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_pyout"&gt;
&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;#image_id&lt;/th&gt;
&lt;th&gt;unixtime&lt;/th&gt;
&lt;th&gt;rawtime&lt;/th&gt;
&lt;th&gt;title&lt;/th&gt;
&lt;th&gt;total_votes&lt;/th&gt;
&lt;th&gt;reddit_id&lt;/th&gt;
&lt;th&gt;number_of_upvotes&lt;/th&gt;
&lt;th&gt;subreddit&lt;/th&gt;
&lt;th&gt;number_of_downvotes&lt;/th&gt;
&lt;th&gt;localtime&lt;/th&gt;
&lt;th&gt;score&lt;/th&gt;
&lt;th&gt;number_of_comments&lt;/th&gt;
&lt;th&gt;username&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt; 0&lt;/td&gt;
&lt;td&gt; 1333172439&lt;/td&gt;
&lt;td&gt; 2012-03-31T12:40:39.590113-07:00&lt;/td&gt;
&lt;td&gt;          And here's a downvote.&lt;/td&gt;
&lt;td&gt; 63470&lt;/td&gt;
&lt;td&gt; rmqjs&lt;/td&gt;
&lt;td&gt; 32657&lt;/td&gt;
&lt;td&gt;    funny&lt;/td&gt;
&lt;td&gt; 30813&lt;/td&gt;
&lt;td&gt; 1333197639&lt;/td&gt;
&lt;td&gt; 1844&lt;/td&gt;
&lt;td&gt; 622&lt;/td&gt;
&lt;td&gt; Animates_Everything&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt; 0&lt;/td&gt;
&lt;td&gt; 1333178161&lt;/td&gt;
&lt;td&gt; 2012-03-31T14:16:01.093638-07:00&lt;/td&gt;
&lt;td&gt;                     Expectation&lt;/td&gt;
&lt;td&gt;    35&lt;/td&gt;
&lt;td&gt; rmun4&lt;/td&gt;
&lt;td&gt;    29&lt;/td&gt;
&lt;td&gt; GifSound&lt;/td&gt;
&lt;td&gt;     6&lt;/td&gt;
&lt;td&gt; 1333203361&lt;/td&gt;
&lt;td&gt;   23&lt;/td&gt;
&lt;td&gt;   3&lt;/td&gt;
&lt;td&gt;       Gangsta_Raper&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt; 0&lt;/td&gt;
&lt;td&gt; 1333199913&lt;/td&gt;
&lt;td&gt; 2012-03-31T20:18:33.192906-07:00&lt;/td&gt;
&lt;td&gt;                        Downvote&lt;/td&gt;
&lt;td&gt;    41&lt;/td&gt;
&lt;td&gt; rna86&lt;/td&gt;
&lt;td&gt;    32&lt;/td&gt;
&lt;td&gt; GifSound&lt;/td&gt;
&lt;td&gt;     9&lt;/td&gt;
&lt;td&gt; 1333225113&lt;/td&gt;
&lt;td&gt;   23&lt;/td&gt;
&lt;td&gt;   0&lt;/td&gt;
&lt;td&gt;       Gangsta_Raper&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt; 0&lt;/td&gt;
&lt;td&gt; 1333252330&lt;/td&gt;
&lt;td&gt;        2012-04-01T10:52:10-07:00&lt;/td&gt;
&lt;td&gt; Every time I downvote something&lt;/td&gt;
&lt;td&gt;    10&lt;/td&gt;
&lt;td&gt; ro7e4&lt;/td&gt;
&lt;td&gt;     6&lt;/td&gt;
&lt;td&gt; GifSound&lt;/td&gt;
&lt;td&gt;     4&lt;/td&gt;
&lt;td&gt; 1333277530&lt;/td&gt;
&lt;td&gt;    2&lt;/td&gt;
&lt;td&gt;   0&lt;/td&gt;
&lt;td&gt;       Gangsta_Raper&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt; 0&lt;/td&gt;
&lt;td&gt; 1333272954&lt;/td&gt;
&lt;td&gt; 2012-04-01T16:35:54.393381-07:00&lt;/td&gt;
&lt;td&gt;  Downvote &amp;amp;quot;Dies Irae&amp;amp;quot;&lt;/td&gt;
&lt;td&gt;    65&lt;/td&gt;
&lt;td&gt; rooof&lt;/td&gt;
&lt;td&gt;    57&lt;/td&gt;
&lt;td&gt; GifSound&lt;/td&gt;
&lt;td&gt;     8&lt;/td&gt;
&lt;td&gt; 1333298154&lt;/td&gt;
&lt;td&gt;   49&lt;/td&gt;
&lt;td&gt;   0&lt;/td&gt;
&lt;td&gt;       Gangsta_Raper&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 13 columns&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Note that I first called &lt;code&gt;csvclean&lt;/code&gt;, from the &lt;a href="http://csvkit.readthedocs.org/en/latest/index.html"&gt;csvkit suite of command line utilities&lt;/a&gt;. The bang (!) symbol calls the command line from IPython. &lt;code&gt;csvclean&lt;/code&gt; fixes a couple of formatting errors in the original dataset that interfere with R/Panda's parsing functions (something to do with quotes or commas in the "title" field, I believe). The repaired CSV is saved with a &lt;code&gt;_out&lt;/code&gt; prepended to the filename. Nothing fancy is required for the &lt;code&gt;read_csv&lt;/code&gt; call in our case.&lt;/p&gt;
&lt;p&gt;Now for the hard/interesting part. How do we map the flow of each image submission through the various subreddits?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First we sort by image and (crucially) timetamp on line 4.&lt;/li&gt;
&lt;li&gt;On line 5, I simply extract the 3 columns that we care about. &lt;/li&gt;
&lt;li&gt;Now we drop resubmissions of each image to the &lt;em&gt;same&lt;/em&gt; subreddit with drop_duplicates on line 6, which only keeps each image's &lt;em&gt;first&lt;/em&gt; submission to a particular subreddit (why we sorted first). &lt;/li&gt;
&lt;li&gt;The last thing we need Pandas for is to group by image ID (line 7).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On line 7, we pull the list of subreddits (now unique and nicely ordered) for each image. The nested list comprehension is necessary only because calling &lt;code&gt;.subreddit&lt;/code&gt; on the groupby object &lt;code&gt;g&lt;/code&gt; returns a tuple by default, and we'd rather have a list of lists.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
In [3]:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="c"&gt;# Identify the order in which each image is submitted to various subreddits, &lt;/span&gt;
&lt;span class="c"&gt;# removing repeats within a subreddit&lt;/span&gt;

&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;'#image_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'unixtime'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;\
     &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[:,[&lt;/span&gt;&lt;span class="s"&gt;'#image_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'unixtime'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'subreddit'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;\
     &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop_duplicates&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'#image_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'subreddit'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;\
     &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'#image_id'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;flow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subreddit&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Now we need a function &lt;code&gt;window&lt;/code&gt; that rolls along each of the lists in &lt;code&gt;flow&lt;/code&gt; and connects every subsequent pair of subreddits that a particular image was submitted to. We'll do this with the help of the wonderful &lt;code&gt;itertools&lt;/code&gt; module, creating two dimensional tuples that encode the "from" subreddit and the "to" subreddit, respectively. In lines 14 and 15, we apply the function and flatten the result to a single list.&lt;/p&gt;
&lt;p&gt;In order to truly capture the "flow," however, we need to distinguish between the "gifs" subreddit node where images are popping up for the first time and the "gifs" subreddit node when the image has already appeared in another subreddit (say, "pics"). The &lt;code&gt;enumerate&lt;/code&gt; in line 14 does this by tacking on the ordinality to the name of the node, admittedly very hacky, but we have a lot of tuples floating around already.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
In [4]:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="c"&gt;# Roll along the list of subreddits each image has been submitted to, &lt;/span&gt;
&lt;span class="c"&gt;# creating an edge tuple for each subsequent pair&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;window&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;'''Returns a sliding window (of width n) over data from the iterable&lt;/span&gt;
&lt;span class="sd"&gt;       s -&amp;gt; (s0,s1,...s[n-1]), (s1,s2,...,sn), ...'''&lt;/span&gt;
    &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;islice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;    
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;elem&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elem&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="n"&gt;sankey&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;)]))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sub&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sankey&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sublist&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sankey&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sublist&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c"&gt;# flatten&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;At last we have a list of edges that on some level describes the flow that we are trying to get at. Now we can just iterate through them and use &lt;code&gt;NetworkX&lt;/code&gt; to create the graph and weight the edges appropriately. In lines 10--13, I prune back the tiny edges that clutter up the diagram, and then the nodes that are no longer associated with any edges. Last but not least, we export the structure to a JSON in line 16.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
In [5]:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="c"&gt;# Create network structure&lt;/span&gt;
&lt;span class="n"&gt;S&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DiGraph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;edge&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sankey&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;has_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;edge&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;edge&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="n"&gt;edge&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="s"&gt;'weight'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;edge&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        
&lt;span class="c"&gt;# Trim edges&lt;/span&gt;
&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remove_edges_from&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;'weight'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;flagged&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out_degree&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s"&gt;'3'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remove_edges_from&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;flagged&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remove_nodes_from&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;degree&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c"&gt;# Export&lt;/span&gt;
&lt;span class="n"&gt;json_graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'sankey.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'w'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Time for R!&lt;/p&gt;
&lt;p&gt;We will need to make sure that &lt;code&gt;d3Network&lt;/code&gt; is installed to the the instance of R that is used by IPython's &lt;code&gt;Rmagic&lt;/code&gt; via &lt;code&gt;Rpy2&lt;/code&gt;. It was different for me (I think?) so if you are running something like this for the first time, include the lines that are commented out.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;%%R&lt;/code&gt; denotes block-level R magicks in IPython (&lt;code&gt;%R&lt;/code&gt; will give you line-level magicks)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
In [6]:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="k"&gt;R&lt;/span&gt;
&lt;span class="c"&gt;#install.packages('devtools')&lt;/span&gt;
&lt;span class="c"&gt;#library(devtools)&lt;/span&gt;
&lt;span class="c"&gt;#devtools::install_github("christophergandrud/d3Network")&lt;/span&gt;
&lt;span class="n"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d3Network&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This is finally the point at which Christopher Gandrud's package simplifies everything for us. We simply read in the nodes and linkes (edges) from the JSON file (they get converted to two dataframes). Note that we have to strip the janky ordinality numbers that we tacked onto the node names (line 3). Now that different nodes have the same names, the package will even make sure that each subreddit node has the same color every time it appears!&lt;/p&gt;
&lt;p&gt;The call to &lt;code&gt;d3Sankey&lt;/code&gt; points to the the nodes dataframe, the links dataframe, the name of the sources/targets in the links dataframe, the name of the column that holds the link weights, and then some display configuration stuff.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
In [7]:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="k"&gt;R&lt;/span&gt;
&lt;span class="n"&gt;nodes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;JSONtoDF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'sankey.json'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'nodes'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nb"&gt;id&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;substring&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nb"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;links&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;JSONtoDF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'sankey.json'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'links'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;d3Sankey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Links&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;links&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'source'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
         &lt;span class="n"&gt;Target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'target'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'weight'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NodeID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
         &lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;600&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
         &lt;span class="n"&gt;standAlone&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iframe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'sankey.html'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;
Error in withVisible({ : could not find function "JSONtoDF"

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;We can render the resulting iframe directly in our IPython notebook! Hover over edges for some nice brushing or click and drag the nodes to untangle a relationship you're interested in.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;
In [6]:
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight-ipynb"&gt;&lt;pre class="ipynb"&gt;&lt;span class="n"&gt;HTML&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'&amp;lt;iframe src="//gist.githubusercontent.com/justmytwospence/2c4e103693636b645f62/raw/71119ea1b0f427abd5d1d6cd4ec1e1f57e7ebf32/sankey.html" height=540 width=700 frameBorder="0"&amp;gt;&amp;lt;/iframe&amp;gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;&lt;div class="prompt output_prompt"&gt;
    Out[6]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_pyout"&gt;
&lt;iframe frameborder="0" height="540" src="//gist.githubusercontent.com/justmytwospence/2c4e103693636b645f62/raw/71119ea1b0f427abd5d1d6cd4ec1e1f57e7ebf32/sankey.html" width="700"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This type of "tiered" Sankey diagram is a little unconventional, but so far its the best way I can come up with to visualize the interesting phenomenon of submisison flow through Reddit. Leave a comment if this gives you any interesting ideas, I'd love to hear them!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Tue, 13 May 2014 21:11:46 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2014-05-13:sankey/</guid><category>Python</category><category>R</category><category>Sankey diagram</category><category>Pandas</category><category>NetworkX</category><category>D3.js</category></item><item><title>Exploring IMDB movie ratings with Shiny</title><link>http://pelican.spencerboucher.com/imdb-shiny/</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Sat, 26 Apr 2014 00:00:00 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2014-04-26:imdb-shiny/</guid><category>imdb</category><category>shiny</category><category>ggplot2</category></item><item><title>A blogging framework a data scientist could love</title><link>http://pelican.spencerboucher.com/moving-to-pelican/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;It has taken a few weeks of sporadic work, but I've finally dumped my old webhost and WordPress in favor of a static blogging framework. Static blogs are "&lt;em&gt;baked&lt;/em&gt;," meaning they don't have any databases or server-side PHP going on behind the scenes, building the website on-the-fly every time someone requests to view the page. Rather, every time there is an update (new post, new page, etc.), a script is run that goes bakes the entire shebang into nothing more than a bunch of html files. There are a ton of advantages to this paradigm, principally speed and security. The main reason I decided to make the switch, however, is that I now have 100% control over every &lt;code&gt;div&lt;/code&gt;, every script, every &lt;em&gt;anything&lt;/em&gt; on the site. This gives me the ability to write posts in &lt;a href="https://daringfireball.net/projects/markdown/"&gt;markdown&lt;/a&gt; plus tons of flexibility to incorporate hackery things like &lt;a href="http://yihui.name/knitr/"&gt;Knitr&lt;/a&gt; documents, &lt;a href="http://ipython.org/notebook.html"&gt;IPython notenooks&lt;/a&gt;, and even &lt;a href="d3js.org"&gt;D3.js&lt;/a&gt; visualizations. Additionally, because there is no backend required, the blog can now be hosted for free on &lt;a href="https://pages.github.com/"&gt;GitHub Pages&lt;/a&gt;, saving somewhere in the neigborhood of 100 bucks every year. And theres just something so much cooler about publishing via &lt;code&gt;git push&lt;/code&gt; rather than clicking a button in WordPress.  &lt;/p&gt;
&lt;p&gt;Anyways, as far as I can tell, its an unwritten rule that your first blog post after switching to a static blogging framework is &lt;em&gt;required&lt;/em&gt; to be about how you did it. I think this may be of interest to some people because its actually rather involved to set up support for all the types of things that data sciencey people like to do, and I'm doing my best to create a framework that can be used out of the box for all this.  &lt;/p&gt;
&lt;p&gt;The primo static blog generator out there seems to be &lt;a href="jekyllrb.com/"&gt;Jekyll&lt;/a&gt; (its even the one that GitHub Pages seems to recommend). Jekyll seems really nice, but its written in Ruby and I'm more of a Python kinda guy. I actually built a quick and dirty version of the blog in &lt;a href="http://octopress.org/"&gt;Jekyll/Octopress&lt;/a&gt; to see what it was all about, which was a great exercise in learning the basics of Ruby and rbenv, etc., but in the end I opted for the pre-eminent &lt;em&gt;Python&lt;/em&gt; solution: &lt;a href="http://docs.getpelican.com/en/3.3.0/"&gt;Pelican&lt;/a&gt; (an anagram of calepin, meaning "notebook" in French, in case you were wondering). Pelican blogs are highly themeable/customizable, and I knew that I wanted a theme based on &lt;a href="getbootstrap.com"&gt;Twitter Bootstrap&lt;/a&gt;, because I'm familiar with it from building a few &lt;a href="http://shiny.rstudio.com/"&gt;Shiny applications&lt;/a&gt;, and its very ${powerful, simple, systematic, well-documented}. There's a few Bootstrap based Pelican themes out there, but &lt;a href="https://github.com/DandyDev/pelican-bootstrap3"&gt;pelican-bootstrap3&lt;/a&gt; by &lt;a href="http://dandydev.net/"&gt;DandyDev&lt;/a&gt; is my favorite and is based on the most recent Bootstrap release.  &lt;/p&gt;
&lt;p&gt;I've made many aesthetic tweaks, including a full-page width article layout, custom typography via &lt;a href="https://www.google.com/fonts‎"&gt;Google Fonts&lt;/a&gt; and changes to the sidebar/article index/navbar. I've added a few javascript and CSS files grant built-in support for MathJax, &lt;a href="{filename}/R/ggplot-goodness.html"&gt;Knitr documents&lt;/a&gt;, GitHub themed Pygments code blocks, and interactive Bootstrap elements (like the tooltips on the Category section of the sidebar to the right). I'm using the &lt;a href="https://github.com/wrobstory/pelican_dynamic‎"&gt;pelican_dynamic&lt;/a&gt; plugin to make &lt;a href="{filename}/Vizualization/d3-test.html"&gt;D3&lt;/a&gt; and any other arbitrary scripts includeable within individual posts. A modified version of the &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/liquid_tags"&gt;liquid_tags&lt;/a&gt; plugin makes it possible to include &lt;a href="{filename}/Python/contract-optimization.md"&gt;IPython notebooks&lt;/a&gt; or even select pieces of a notebook.  &lt;/p&gt;
&lt;p&gt;It's all very much still a work in progress. As I have time, I will continue to update and document the framework. The hope is that eventually any data scientist with limited front-end knowledge (ie &lt;em&gt;me!&lt;/em&gt;) can start publishing their work in one place within 20 minutes.  If anyone has thoughts or comments please comment! Pull requests would be very welcome but heads up because I'm still new to the world of collaborative open-source.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class="text-center"&gt;
&lt;a class="btn btn-primary btn-lg" href="http://github.com/justmytwospence/pelican" target="_blank" type="button"&gt;
        Check out the framework on GitHub
    &lt;/a&gt;
&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Sat, 05 Apr 2014 00:00:00 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2014-04-05:moving-to-pelican/</guid><category>blogging</category><category>pelican</category><category>bootstrap</category><category>design</category><category>knitr</category><category>ipython</category><category>ipython notebook</category><category>mathjax</category><category>d3.js</category></item><item><title>D3 Test</title><link>http://pelican.spencerboucher.com/d3-test/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;To test my ability to embed D3.js visualizations into my blog, I present to you basically my first D3 graph. Its basically the one that Scott Murray walks you through in his book &lt;a href="http://chimera.labs.oreilly.com/books/1230000000345" target="_blank"&gt;Interactive Data Visualization for the Web&lt;/a&gt;. I highly recommend it for all skill levels!&lt;/p&gt;
&lt;div style="text-align: center; margin-top: 20px"&gt;
&lt;div class="btn-group btn-group-lg text-center"&gt;
&lt;button class="btn btn-primary" id="add"&gt;Add&lt;/button&gt;
&lt;button class="btn btn-primary" id="remove"&gt;Remove&lt;/button&gt;
&lt;/div&gt;
&lt;div id="svg"&gt;&lt;/div&gt;
&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Fri, 21 Mar 2014 00:00:00 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2014-03-21:d3-test/</guid><category>d3.js</category></item><item><title>What's a Mooc?</title><link>http://pelican.spencerboucher.com/whats-a-mooc/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;Turns out that Robert de Niro and friends are facing a problem very  similar to the one facing Coursera and friends...&lt;/p&gt;
&lt;hr/&gt;
&lt;div class="video-container"&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="315" src="//www.youtube.com/embed/TswwON_Hs1M" width="560"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;p&gt;From the 1973 movie &lt;a href="http://www.imdb.com/title/tt0070379/?ref_=fn_al_tt_1"&gt;Mean Streets&lt;/a&gt;. Apparently Scorsese predicted the education revolution wayy ahead of his time and had a pretty good grasp on the biggest challenges that MOOCs would need to overcome.    &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Fri, 14 Mar 2014 14:13:00 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2014-03-14:whats-a-mooc/</guid><category>Coursera</category><category>education</category><category>MOOC</category></item><item><title>writeLaTeX</title><link>http://pelican.spencerboucher.com/writelatex/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;If you use $\LaTeX$ and haven't yet heard of
$write\LaTeX$, do yourself a favor and check it out. On one level
its a really great version of Google Docs for documents that are
properly typeset, which is incredibly useful because a huge swathe of
the documents created with $\LaTeX$ are inherently collaborative in
nature. Its other fantastic feature is that your $\LaTeX$ markup is
automatically rendered in real-time (or close to it, at least, there's a
few seconds of lag depending on the length of the document). This made
writing my first $\LaTeX$ intensive document a great experience,
because I could experiment liberally with equations and figure
placement. There's literally zero barrier to entry because its
completely web based; you don't need to install a thing and templates
are available to get you jump-started. Right now other engines like
XeLaTeX aren't supported, but I believe they are in the works.  &lt;/p&gt;
&lt;p&gt;So every excuse you ever had to not learn $\LaTeX$ has been
obliterated. &lt;a href="http://writelatex.com"&gt;Give it a try&lt;/a&gt;, or just be passive aggressive
and send it to that collaborator that always sends you everything in a
poorly formatted Word document. I myself will be experimenting with
using $\LaTeX$ to take math-heavy notes in real-time, which sounds
crazy but the live rendering makes the attempt feasible.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Sat, 25 Jan 2014 18:42:00 -0800</pubDate><guid>tag:pelican.spencerboucher.com,2014-01-25:writelatex/</guid><category>latex</category><category>tex</category><category>typesetting</category><category>writelatex</category></item><item><title>Scheduling tasks in the cloud with EC2 APIs</title><link>http://pelican.spencerboucher.com/ec2-apis/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;This post is sort of an addendum to our &lt;a href="http://www.spencerboucher.com/live-mapping/" title="Live mapping"&gt;live-mapping project&lt;/a&gt;, but it should also be of use to anyone looking to run an arbitrary script on a recurring schedule. Originally, we set up a 24/7 instance on &lt;a href="http://aws.amazon.com/ec2/"&gt;Amazon's Elastic Compute Cloud&lt;/a&gt; that ran a daily &lt;code&gt;cron&lt;/code&gt; job. This works, but its a bit wasteful because we're paying for 24 hours of cloud even though we're only actually using it for maybe 5 minutes a day.  &lt;/p&gt;
&lt;p&gt;Fortunately, Amazon provides a &lt;a href="http://aws.amazon.com/developertools/"&gt;schmorgesborg&lt;/a&gt; of command line interface (CLI) tools that allow us to manage our cloud instances more efficiently. Specifically, we want to schedule an instance to spin up only once a day, execute our script, then shut back down. To accomplish this, we will want three CLI tools: &lt;a href="http://aws.amazon.com/developertools/368"&gt;the Amazon EC2 AMI Tools&lt;/a&gt;, &lt;a href="http://aws.amazon.com/developertools/351"&gt;the Amazon EC2 API Tools&lt;/a&gt;,and &lt;a href="http://aws.amazon.com/developertools/2535"&gt;the Auto Scaling Command Line Tool&lt;/a&gt;. If you're on a Mac, it's way easier to get these with &lt;a href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; than by downloading from Amazon's website:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;brew install ec2-ami-tools &lt;span class="c"&gt;# For creating an AMI from an existing machine&lt;/span&gt;
brew install ec2-api-tools &lt;span class="c"&gt;# For registering and launching instances&lt;/span&gt;
brew install aws-as        &lt;span class="c"&gt;# For creating auto scaling groups / defining schedules&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As an extra Homebrew bonus, running &lt;code&gt;brew info ec2-ami-tools&lt;/code&gt;, &lt;code&gt;brew info ec2-api-tools&lt;/code&gt;, and &lt;code&gt;brew info aws-as&lt;/code&gt; will now tell us exactly what we need to do to get our authentication and environment variables all set up. First we are told to download the necessary .pem files from &lt;a href="http://aws-portal.amazon.com/gp/aws/developer/account/index.html?action=access-key"&gt;this Amazon page&lt;/a&gt; and place them into a new hidden directory of our home directory &lt;code&gt;.ec2&lt;/code&gt;. Then we tell our command line where everything lives now by inserting the following lines into our &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_PRIVATE_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"$(/bin/ls "&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;"/.ec2/pk-*.pem | /usr/bin/head -1)"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_CERT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"$(/bin/ls "&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;"/.ec2/cert-*.pem | /usr/bin/head -1)"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/usr/local/Cellar/ec2-api-tools/1.6.12.0/libexec"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;AWS_AUTO_SCALING_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/usr/local/Cellar/auto-scaling/1.0.61.4/libexec"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_AMITOOL_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/usr/local/Cellar/ec2-ami-tools/1.4.0.9/libexec"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_REGION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"us-west-2"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_ZONE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EC2_REGION&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;a
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;https://&lt;span class="nv"&gt;$EC2_REGION&lt;/span&gt;.ec2.amazonaws.com
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;AWS_AUTO_SCALING_URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;https://autoscaling.&lt;span class="nv"&gt;$EC2_REGION&lt;/span&gt;.amazonaws.com
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Its pretty simple, but if you have any trouble with this part, refer to the official &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/SettingUp_CommandLine.html"&gt;Amazon documentation for setting up the command line&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;Because these environment variables are recognized out of the box by the CLI tools, we won't need to point to our authentication keys or specify a region every time we make an API call and our next commands will be much more succinct. Note that every EC2 instance is physically located at one of several regions; we are using us-west-2 because it happens to be where I spun up the existing instance that currently holds our "update.py" script, but any of them would probably work just fine for the simple job at hand.  &lt;/p&gt;
&lt;div class="table-responsive"&gt;
&lt;table class="table table-striped table-condensed "&gt;
&lt;tr&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;th&gt;Region&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ap-northeast-1&lt;/td&gt;
&lt;td&gt; Asia Pacific (Tokyo) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ap-southeast-1&lt;/td&gt;
&lt;td&gt;Asia Pacific (Singapore) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ap-southeast-2&lt;/td&gt;
&lt;td&gt;Asia Pacific (Sydney) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;eu-west-1&lt;/td&gt;
&lt;td&gt;EU (Ireland) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sa-east-1&lt;/td&gt;
&lt;td&gt;South America (Sao Paulo) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;us-east-1&lt;/td&gt;
&lt;td&gt;US East (Northern Virginia) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;us-west-1&lt;/td&gt;
&lt;td&gt;US West (Northern California) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;us-west-2&lt;/td&gt;
&lt;td&gt;US West (Oregon) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;So, first things first. We can't just spin up an off-the-rack EC2 instance every day, because we'll run into the same problem that I originally had with my web host: the Python modules that we need won't be installed. We &lt;em&gt;could&lt;/em&gt; write a script that would install &lt;code&gt;pip&lt;/code&gt; plus all of the requisite Python modules and run it first thing after we launch the instance, but there's a better way:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ec2-create-image i-8918e1be -n &lt;span class="s2"&gt;"Map Update Image"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This command from &lt;code&gt;ec2-ami-tools&lt;/code&gt; creates an "Amazon Machine Image" of the instance that we previously had running and names it "Map Update Image". A new image ID will now print to your console, &lt;code&gt;ami-fcdfb9cc&lt;/code&gt; in my case. This is tantamount to cloning the instance, because we can now reference the new image ID when we spin up new instances and all of our modules, scripts, etc. will be there waiting for us. Note that I removed the instance's &lt;code&gt;cron&lt;/code&gt; job &lt;em&gt;before&lt;/em&gt; creating the AMI, because we'll now be handling the task scheduling from &lt;em&gt;outside&lt;/em&gt; the instance, via &lt;strong&gt;autoscaling&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;Next let's write a shell script that will execute our Python map-updating script, shoot us a diagnostic email, then shut down the instance that its running on. The idea here is that once a day we're going to spin up an instance using our shiny new AMI and immediately run this new script (let's call it "update.sh") that will do its business and then promptly commit seppuku and stop charging us money. Eric Hammond has created a great template on &lt;a href="http://alestic.com/2011/11/ec2-schedule-instance"&gt;his blog&lt;/a&gt;, which I've modified below. Note the execution of our &lt;a href="http://www.spencerboucher.com/live-mapping/" title="Live mapping"&gt;familiar&lt;/a&gt; "update.py" script highlighted on line 4, and the apoptosis command on line 46:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash -x&lt;/span&gt;
&lt;span class="nb"&gt;exec&lt;/span&gt; &amp;gt; &amp;gt;&lt;span class="o"&gt;(&lt;/span&gt;tee /var/log/user-data.log|logger -t user-data -s 2&amp;gt;/dev/console&lt;span class="o"&gt;)&lt;/span&gt; 2&amp;gt;&amp;amp;1

/usr/bin/python /home/ubuntu/update.py &lt;span class="c"&gt;# Run the script&lt;/span&gt;

&lt;span class="nv"&gt;EMAIL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;spencer.g.boucher@gmail.com

&lt;span class="c"&gt;# Upgrade and install Postfix so we can send a sample email&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;DEBIAN_FRONTEND&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;noninteractive
apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get upgrade -y &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y postfix

&lt;span class="c"&gt;# Get some information about the running instance&lt;/span&gt;
&lt;span class="nv"&gt;instance_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;wget -qO- instance-data/latest/meta-data/instance-id&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;public_ip&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;wget -qO- instance-data/latest/meta-data/public-ipv4&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;zone&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;wget -qO- instance-data/latest/meta-data/placement/availability-zone&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;region&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;expr match &lt;span class="nv"&gt;$zone&lt;/span&gt; &lt;span class="s1"&gt;'\(.*\).'&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;uptime&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;uptime&lt;span class="k"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Send status email&lt;/span&gt;
/usr/sbin/sendmail -oi -t -f &lt;span class="nv"&gt;$EMAIL&lt;/span&gt; &lt;span class="s"&gt;&amp;lt;&amp;lt;EOM&lt;/span&gt;
&lt;span class="s"&gt;From: $EMAIL&lt;/span&gt;
&lt;span class="s"&gt;To: $EMAIL&lt;/span&gt;
&lt;span class="s"&gt;Subject: Results of EC2 scheduled script&lt;/span&gt;

&lt;span class="s"&gt;This email message was generated on the following EC2 instance:&lt;/span&gt;

&lt;span class="s"&gt;  instance id: $instance_id&lt;/span&gt;
&lt;span class="s"&gt;  region:      $region&lt;/span&gt;
&lt;span class="s"&gt;  public ip:   $public_ip&lt;/span&gt;
&lt;span class="s"&gt;  uptime:      $uptime&lt;/span&gt;

&lt;span class="s"&gt;If the instance is still running, you can monitor the output of this&lt;/span&gt;
&lt;span class="s"&gt;job using a command like:&lt;/span&gt;

&lt;span class="s"&gt;  ssh ubuntu@$public_ip tail -1000f /var/log/user-data.log&lt;/span&gt;

&lt;span class="s"&gt;  ec2-describe-instances --region $region $instance_id&lt;/span&gt;

&lt;span class="s"&gt;EOM&lt;/span&gt;

&lt;span class="c"&gt;# Give the script and email some time to do their thing&lt;/span&gt;
sleep 600 &lt;span class="c"&gt;# 10 minutes&lt;/span&gt;

&lt;span class="c"&gt;# This will stop the EBS boot instance, stopping the hourly charges.&lt;/span&gt;
&lt;span class="c"&gt;# Have Auto Scaling terminate it, stopping the storage charges.&lt;/span&gt;
shutdown -h now

&lt;span class="nb"&gt;exit &lt;/span&gt;0
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the user data script that we pass to the launch configuration executes with &lt;em&gt;root&lt;/em&gt; permissions, not as the user "ubuntu" that you would typically log in as via &lt;code&gt;ssh&lt;/code&gt;. Its probably best to be as explicit as possible when specifying path names in the cloud, the tilde operator might turn around and bite you.  &lt;/p&gt;
&lt;p&gt;Now we need to create &lt;strong&gt;launch configuration&lt;/strong&gt; that will basically do all the button-pushing that we would normally be doing at the AWS console GUI.  &lt;/p&gt;
&lt;p&gt;Here we specify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"Micro" as our instance type.&lt;/li&gt;
&lt;li&gt;Our shell script "update.sh" from step 2 as the "user-data-file". User data files are passed into the instance and executed immediately when supplied in the launch configuration. They must be less than 16kb as I suppose they are stored on some ancillary server somewhere.&lt;/li&gt;
&lt;li&gt;The AMI image that we cloned in step 1 from the instance that included our Python modules.&lt;/li&gt;
&lt;li&gt;The name of the launch config; let's call it "map-update-launch-config".&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;as-create-launch-config
   --instance-type t1.micro
   --user-data-file ~/Desktop/update.sh
   --image-id ami-fcdfb9cc
   --launch-config &lt;span class="s2"&gt;"map-update-launch-config"&lt;/span&gt;
as-describe-launch-configs --headers
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the second line provides a list of all the launch configurations that have been created.  &lt;/p&gt;
&lt;p&gt;We must also create an &lt;strong&gt;auto scaling group&lt;/strong&gt;. These are typically used as a sort of container to which we can add/remove instances on a schedule or in response to heavy traffic, but we can also use it to schedule a single instance to flick on and off. We need to tell it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A name to assign the scaling group ("map-update-scale-group").&lt;/li&gt;
&lt;li&gt;The name of the launch configuration we created in step 3 ("map-update-launch-config").&lt;/li&gt;
&lt;li&gt;Which availability zone we want to use (basically irrelevant; we set our environment variable &lt;code&gt;EC2_ZONE&lt;/code&gt; to "a" earlier). &lt;code&gt;ec2-describe-available-zones&lt;/code&gt; provides a list of the available zones&lt;/li&gt;
&lt;li&gt;A minimum and maximum number of instances in the group. We'll initialize these to zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;as-create-auto-scaling-group
   --auto-scaling-group &lt;span class="s2"&gt;"map-update-scale-group"&lt;/span&gt;
   --launch-configuration &lt;span class="s2"&gt;"map-update-launch-config"&lt;/span&gt;
   --availability-zones &lt;span class="s2"&gt;"$EC2_ZONE"&lt;/span&gt;
   --min-size 0 --max-size 0
as-suspend-processes &lt;span class="s2"&gt;"map-update-scale-group"&lt;/span&gt;
   --processes ReplaceUnhealthy
as-describe-auto-scaling-groups --headers
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the second line, we are using &lt;code&gt;as-suspend-processes&lt;/code&gt; to prevent the instance's default behavior which is to attempt to restart after it is shut down. The third line provides a list of all the auto scaling groups that have been created.  &lt;/p&gt;
&lt;p&gt;Last but not least, we are ready to assign a schedule to our auto scaling group. Here we create two: one to start the instance and one to terminate the instance. Astute readers will recall that "update.sh" already &lt;em&gt;stops&lt;/em&gt; the instance so that we aren't paying to have it running, but we also need to completely &lt;em&gt;terminate&lt;/em&gt; the instance so that we aren't paying to store information about it. Each schedule requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A name ("map-update-start" &amp;amp; "map-update-stop").&lt;/li&gt;
&lt;li&gt;The name of the auto scaling group we created in step 4 ("map-update-scale-group").&lt;/li&gt;
&lt;li&gt;How we want to scale. By setting both &lt;code&gt;min-size&lt;/code&gt; and &lt;code&gt;max-size&lt;/code&gt; to 1, we are effectively turning on one instance. We later effectively turn that instance back off by setting both to 0.&lt;/li&gt;
&lt;li&gt;A "recurrence," ie when to occur. This flag uses the same syntax that &lt;code&gt;cron&lt;/code&gt; does. Here we set the instance to launch at midnight UTC (&lt;code&gt;0 0 * * *&lt;/code&gt;), and terminate 15 minutes later (&lt;code&gt;15 0 * * *&lt;/code&gt;). Recall that our script already stops the instance 10 minutes after execution, so 15 minutes is playing it safe.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;as-put-scheduled-update-group-action
   --name &lt;span class="s2"&gt;"map-update-start"&lt;/span&gt;
   --auto-scaling-group &lt;span class="s2"&gt;"map-update-scale-group"&lt;/span&gt;
   --min-size 1 --max-size 1
   --recurrence &lt;span class="s2"&gt;"0 0 * * *"&lt;/span&gt;
as-put-scheduled-update-group-action
   --name &lt;span class="s2"&gt;"map-update-stop"&lt;/span&gt;
   --auto-scaling-group &lt;span class="s2"&gt;"map-update-scale-group"&lt;/span&gt;
   --min-size 0 --max-size 0
   --recurrence &lt;span class="s2"&gt;"15 0 * * *"&lt;/span&gt;
as-describe-scheduled-actions --headers
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As before, the third line provides a list of the actions that have been scheduled.  &lt;/p&gt;
&lt;p&gt;And thats it! We are now only paying for 10 or 15 minutes of cloud per day, as opposed to 1,440 of them. To review the timeline we have created in this example: our auto scaling group boots up an instance up at midnight UTC that immediately executes "update.sh". This automatically executes "update.py" and shoots us a diagnostic email. It then waits 10 minutes to make sure everything has time to run, before stopping the instance. 5 minutes after &lt;em&gt;that&lt;/em&gt; the auto scaling group then completely terminates the instance.  &lt;/p&gt;
&lt;p&gt;Other great resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/schedule_time.html"&gt;Official Amazon documentation for scheduling auto scaling groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://alestic.com/2011/11/ec2-schedule-instance"&gt;Running EC2 Instances on a Recurring Schedule with Auto
    Scaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robertsindall.co.uk/blog/how-to-use-amazons-auto-scaling-groups/"&gt;Summary of API commands&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cardinalpath.com/autoscaling-your-website-with-amazon-web-services-part-2/"&gt;Auto Scaling Your Website with Amazon Web Services&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Sun, 12 Jan 2014 17:53:00 -0800</pubDate><guid>tag:pelican.spencerboucher.com,2014-01-12:ec2-apis/</guid><category>api</category><category>auto scaling</category><category>aws</category><category>cloud</category><category>cron</category><category>ec2</category></item><item><title>Titanic: Getting Started With R</title><link>http://pelican.spencerboucher.com/titanic-getting-started-with-r/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;My friend and classmate &lt;a href="http://trevorstephens.com"&gt;Trevor Stephens&lt;/a&gt; has created some &lt;a href="http://trevorstephens.com/post/72916401642/titanic-getting-started-with-r"&gt;pretty stellar R tutorials&lt;/a&gt; that will take you to about halfway up the leaderboard of Kaggle's &lt;a href="http://www.kaggle.com/c/titanic-gettingStarted"&gt;Titanic: Machine Learning from Disaster&lt;/a&gt; competition. While the competition has a &lt;a href="http://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-python"&gt;Python tutorial&lt;/a&gt; and even a beginner's &lt;a href="http://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-excel"&gt;Excel tutorial&lt;/a&gt;, any R equivalent had been suspiciously lacking. This is the competition that served as our first foray into machine learning, so kudos to Trevor for giving back to the community!&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Sat, 11 Jan 2014 22:05:00 -0800</pubDate><guid>tag:pelican.spencerboucher.com,2014-01-11:titanic-getting-started-with-r/</guid><category>kaggle</category><category>machine learning</category><category>R</category></item><item><title>Stratified sampling in R</title><link>http://pelican.spencerboucher.com/stratified-sampling-in-r/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;I was surprised to find that R doesn’t have a base function for stratified random sampling. There’s not even a well known package I could find that does this in a straight forward way. So heres my own.  &lt;/p&gt;
&lt;p&gt;It is essentially a wrapper for a ddply call that samples each subset and then combines them. If the size argument is less than 1, it will be interpreted as the percentage of each stratification subset that should be sampled. If the size argument is greater than 1, it will be interpreted as the number of observations to sample from each stratification subset.  &lt;/p&gt;
&lt;p&gt;Note that in the first case, a different number of observations will be taken from each subset depending on their total number of observations. In the second case however, an equal number of observations will be sampled from each subset, regardless of their total number of observations.  &lt;/p&gt;
&lt;p&gt;The .by argument is formulated the same way it is for any other ddply call.  &lt;/p&gt;
&lt;script src="https://gist.github.com/justmytwospence/7937389.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Fri, 10 Jan 2014 00:00:00 -0800</pubDate><guid>tag:pelican.spencerboucher.com,2014-01-10:stratified-sampling-in-r/</guid><category>R</category><category>sampling</category></item><item><title>Live mapping</title><link>http://pelican.spencerboucher.com/live-mapping/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;I've been wanting to do some more mapping stuff since my first encounter with Leaflet a month or two ago while I was working on a project for &lt;a href="http://auto-grid.com"&gt;AutoGrid&lt;/a&gt;. I had my eye on CartoDB's time series library, &lt;a href="https://github.com/cartodb/torque"&gt;Torque&lt;/a&gt;, because I had really wanted to do some time-series visualization, but time constraints and privacy issues with uploading data to CartoDB's servers prevented me from really exploring. Since I had a few days of free time over winter break, I played around with it a bit and came up with this: &lt;a href="http://www.spencerboucher.com/map"&gt;spencerboucher.com/map&lt;/a&gt;. How'd I do it?  &lt;/p&gt;
&lt;p&gt;First I needed some geographic data, so I turned to a source of data I've been collected for almost a year - my own location. &lt;a href="http://openpaths.cc"&gt;OpenPaths&lt;/a&gt; is a mobile app that records your location at regular time intervals. I opted for every 30 minutes at first, then upped it to every 15 minutes when I discovered that the effect on battery life wasn't nearly as bad as I expected it to be. OpenPaths is a project of &lt;a href="http://nytlabs.com/"&gt;the R&amp;amp;D department at The New York Times&lt;/a&gt; and they &lt;a href="https://openpaths.cc/FAQ"&gt;claim&lt;/a&gt; that you are the only one with access to the collected data. Interestingly, you can grant various &lt;a href="https://openpaths.cc/projects"&gt;research programs&lt;/a&gt; access to your data at your own discretion. Your data is conveniently downloadable as a csv, json, or kml file, so I easily pulled my dataset of \~3,000 time points since December 2012. Unfortunately, I made the switch from iPhone to Android around April (well, that part is fortunate), and forgot to re-download the app, so I only really have data from the around the first three months and last two months of 2013.  &lt;/p&gt;
&lt;p&gt;Turns out, making impressive maps with CartoDB is almost embarrassingly easy. Their GUI is pretty intuitive and running queries on their postgreSQL database is simple. Even time series stuff built on the Torque backend is really just point and click. I decided that the best way to visualize this data was with an aggregated hexbin heatmap of all my past locations, overlaid with a point-by-point replay with a time-slider. From there, it was just a one-line API call to host the map on my website (line 30 highlighted below), which is significantly easier than the legwork that went into crafting a Leaflet map "manually."&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;html&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;head&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;meta&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;"viewport"&lt;/span&gt; &lt;span class="na"&gt;content=&lt;/span&gt;&lt;span class="s"&gt;"initial-scale=1.0, user-scalable=no"&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;title&amp;gt;&lt;/span&gt;Location | Spencer&lt;span class="nt"&gt;&amp;lt;/title&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;link&lt;/span&gt; &lt;span class="na"&gt;rel=&lt;/span&gt;&lt;span class="s"&gt;"shortcut icon"&lt;/span&gt; &lt;span class="na"&gt;href=&lt;/span&gt;&lt;span class="s"&gt;"http://spencerboucher.com/map/favicon.png"&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;link&lt;/span&gt; &lt;span class="na"&gt;rel=&lt;/span&gt;&lt;span class="s"&gt;"stylesheet"&lt;/span&gt; &lt;span class="na"&gt;href=&lt;/span&gt;&lt;span class="s"&gt;"http://libs.cartocdn.com/cartodb.js/v3/themes/css/cartodb.css"&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="c"&gt;&amp;lt;!--[if lte IE 8]&amp;gt;&lt;/span&gt;
&lt;span class="c"&gt;    &amp;lt;link rel="stylesheet" href="http://libs.cartocdn.com/cartodb.js/v3/themes/css/cartodb.ie.css" /&amp;gt;&lt;/span&gt;
&lt;span class="c"&gt;  &amp;lt;![endif]--&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;style &lt;/span&gt;&lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;"text/css"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;html&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;body&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;#map&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="k"&gt;margin&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
          &lt;span class="k"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
          &lt;span class="k"&gt;width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
          &lt;span class="k"&gt;height&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
          &lt;span class="k"&gt;background&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;black&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="nf"&gt;#cartodb-gmaps-attribution&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;visibility&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="k"&gt;hidden&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/style&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;script &lt;/span&gt;&lt;span class="na"&gt;src=&lt;/span&gt;&lt;span class="s"&gt;"http://maps.google.com/maps/api/js?v=3.2&amp;amp;sensor=false"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;script &lt;/span&gt;&lt;span class="na"&gt;src=&lt;/span&gt;&lt;span class="s"&gt;"http://libs.cartocdn.com/cartodb.js/v3/cartodb.js"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
    &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
      &lt;span class="nx"&gt;cartodb&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;createVis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'map'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'http://justmytwospence.cartodb.com/api/v2/viz/e8fd87d0-78b3-11e3-a9e9-e7941b6e2df0/viz.json'&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;body&lt;/span&gt; &lt;span class="na"&gt;onload=&lt;/span&gt;&lt;span class="s"&gt;"init()"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;id=&lt;/span&gt;&lt;span class="s"&gt;'map'&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is pretty awesome, but in light of how easy it all was, I was almost disappointed. Can we take it one step further? Let's put on our &lt;a href="http://quantifiedself.com/about/"&gt;Quantified Self&lt;/a&gt; hats and set about to make this map &lt;em&gt;live&lt;/em&gt;. There's three components to making this happen, so we'll step through them one at a time. First we need to access the most recent data from OpenPaths (there's an API for that!), and then we need to insert that data into CartoDB's database (guess what, there's an API for that too). Last but not least, we need to schedule that data transplant to occur on a regular basis. The Unix utility &lt;code&gt;cron&lt;/code&gt; is the canonical tool for this type of thing, so this seemed like a good time to learn how to use it.  &lt;/p&gt;
&lt;p&gt;Python has a reputation for being a great "glue" language, so that's what we'll use to build this script.  &lt;/p&gt;
&lt;p&gt;Programmatically accessing your data from OpenPaths is super simple. This piece of our script is pulled more or less verbatim from &lt;a href="https://openpaths.cc/api"&gt;the OpenPaths API documentation&lt;/a&gt;. Line 21 (highlighted below) is key - this is where we specify which data you want to pull for injection into the CartoDB database. Here we will grab the last 24 hours of data (\~96 readings, if you're collecting every 15 minutes like me), getting the results in a nice  JSON-formatted variable named &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;urllib&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="n"&gt;ACCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'redacted'&lt;/span&gt;
&lt;span class="n"&gt;SECRET&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'redacted'&lt;/span&gt;
&lt;span class="n"&gt;URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'https://openpaths.cc/api/1'&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build_auth_header&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s"&gt;'oauth_version'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"1.0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;'oauth_nonce'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate_nonce&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="s"&gt;'oauth_timestamp'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;consumer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Consumer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ACCESS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;secret&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SECRET&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'oauth_consumer_key'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;consumer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; 
    &lt;span class="n"&gt;request&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;signature_method&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SignatureMethod_HMAC_SHA1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign_request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;signature_method&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;consumer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_header&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;'start_time'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'end_time'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c"&gt;# get the last 24 hours&lt;/span&gt;
&lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;?&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;URL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;urllib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;urlencode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c"&gt;#print(query)&lt;/span&gt;
&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;request&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;build_auth_header&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;URL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'GET'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;readlines&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HTTPError&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we need to get our new &lt;code&gt;data&lt;/code&gt; variable into CartoDB's postgreSQL server. &lt;a href="http://developers.cartodb.com/documentation/sql-api.html"&gt;CartoDB's SQL API documentation&lt;/a&gt; makes this possible, and there's even a &lt;a href="https://github.com/vizzuality/cartodb-python"&gt;python module&lt;/a&gt; that wraps OAuth2 to simplify things. Although its still in the early stages of development, this module works fine for our current purposes; all we have to do is send it a string that holds the SQL query we want to run. So now we'll just write a for-loop that successively builds an &lt;code&gt;INSERT&lt;/code&gt; query for each element in &lt;code&gt;data&lt;/code&gt; (lines 18-20 highlighted below).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;cartodb&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CartoDBException&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CartoDBAPIKey&lt;/span&gt;

&lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="s"&gt;'spencer.g.boucher@gmail.com'&lt;/span&gt;
&lt;span class="n"&gt;password&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="s"&gt;'redacted'&lt;/span&gt;
&lt;span class="n"&gt;cartodb_domain&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'justmytwospence'&lt;/span&gt;
&lt;span class="n"&gt;API_KEY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'redacted'&lt;/span&gt;
&lt;span class="n"&gt;cl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CartoDBAPIKey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;API_KEY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cartodb_domain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;reading&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;alt&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'alt'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;device&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt;     &lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'device'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;lat&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'lat'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;lon&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'lon'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;os&lt;/span&gt;      &lt;span class="o"&gt;=&lt;/span&gt;     &lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'os'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt;       &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'t'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;version&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;     &lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'version'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;query_string&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"INSERT INTO openpaths_justmytwospence (alt, date, device, lat,  lon, os, version, the_geom) "&lt;/span&gt;
                       &lt;span class="s"&gt;"VALUES ({0}, abstime({1}), '{2}', {3}, {4}, '{5}', '{6}', ST_ SetSRID(ST_Point({4}, {3}), 4326))"&lt;/span&gt;
                      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;cl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query_string&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;CartoDBException&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"some error ocurred"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A few notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It would certainly be faster to insert all of the new data into the
    database using a single &lt;code&gt;INSERT&lt;/code&gt; statement, but that would require
    some more tedious text parsing and execution speed isn't
    particularly important to us. As it stands, it takes about six
    seconds to post a day's worth of data.&lt;/li&gt;
&lt;li&gt;One posgreSQL "gotcha" had me hung up for quite some time: single
    quotes parse fine but double quotes do not.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ST_SetSRID&lt;/code&gt; is a &lt;a href="http://postgis.org/docs/ST_SetSRID.html"&gt;PostGIS command&lt;/a&gt; that converts a lon/lat pair
    (in that order - another "gotcha") to the necessary geometry object.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Last but not least, we need this script to run automatically. Because we've written the script to transplant 24 hours of data, we'll need to run it once a day in order to capture all of the data that's being generated. I tried to set up my web host, &lt;a href="https://laughingsquid.us/"&gt;LaughingSquid&lt;/a&gt;, to do this, but unfortunately they don't grant shell access so we can't install all those fancy python modules that we've already used. Its totally possible to rewrite the script to use only modules from the &lt;a href="http://docs.python.org/2/library/"&gt;Python Standard Library&lt;/a&gt;, but this would turn a simple task into a tedious one. Manually implementing OAuth in particular would be a total pain in the rear, and classes are just about to resume after all, so a different solution is in order. Let's spin up a &lt;a href="http://aws.amazon.com/"&gt;"micro" EC2 instance&lt;/a&gt; instead. This gives us free reign to install whatever we need for the low low cost of ¢.02 per hour. This does start to add up, but our Master's program gives us some pretty substantial Amazon Web Services credit that goes mostly unused, so we aren't too upset :). UPDATE: A new post provides details about how to schedule Amazon EC2 instances - &lt;a href="http://www.spencerboucher.com/ec2-apis/"&gt;http://www.spencerboucher.com/ec2-apis/&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;After &lt;code&gt;pip install&lt;/code&gt;ing everything we need and &lt;code&gt;scp&lt;/code&gt;ing our python script (let's call it update.py) into the home directory of our remote server, all we need to do is set up a crontab with the &lt;code&gt;crontab -e&lt;/code&gt; command and add the following line:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;@daily /usr/bin/python ~/update.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;@daily&lt;/code&gt; is actually a shortcut for &lt;code&gt;* * * * *&lt;/code&gt;, where each asterix is a
placeholder for the (respectively) minute, hour, day of month, month,
and day of week that the script should executed. This shortcut defaults
to midnight every day, which is really as good as anything for our
purposes.  &lt;/p&gt;
&lt;p&gt;Voilà! Now we can step back and relax, knowing that we don't have to do a single thing and our map will continue to show the most up-to-date data available.  &lt;/p&gt;
&lt;p&gt;A few final notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We might reasonably want to lag our script by a week or so, for security/privacy reasons.&lt;/li&gt;
&lt;li&gt;As far as I can tell, the location readings are recorded in a &lt;a href="http://en.wikipedia.org/wiki/Unix_time"&gt;POSIX time&lt;/a&gt; and have not been adjusted by time zone, so they are still in the &lt;a href="http://en.wikipedia.org/wiki/Coordinated_Universal_Time"&gt;UTC&lt;/a&gt; time zone. This means that they are 8 hours off from the actual time in California, where I usually am. This doesn't bother me too much at the moment because the visualization is still relatively low resolution in the time domain anyways. At some point I might implement the relevant transformation, but this will raise its own issues because I won't &lt;em&gt;always&lt;/em&gt; be in California, not to mention all that Daylight Savings nonsense.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.spencerboucher.com/ec2-apis/"&gt;Click here for an addendum to this post that will take you through how to schedule the EC2 instance and avoid having it run 24/7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Wed, 08 Jan 2014 08:50:00 -0800</pubDate><guid>tag:pelican.spencerboucher.com,2014-01-08:live-mapping/</guid><category>api</category><category>aws</category><category>cartoDB</category><category>cartography</category><category>cron</category><category>ec2</category><category>map</category><category>quantifiedSelf</category></item><item><title>Map-time at Stamen</title><link>http://pelican.spencerboucher.com/map-time-at-stamen/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;Last week, a classmate and I took a break from coursework to attend one of the many great Meetup events that San Francisco has to offer for data science practitioners. I’ve been pushing myself to attend at least one data-centric Meetup every week, because these events are one of the most amazing parts about going to school in the same place where so many of the biggest names work. To be honest, I believe that becoming a presence in the data science scene and meeting the movers and shakers is equally if not more important than coursework.  &lt;/p&gt;
&lt;p&gt;I actually attended 3 meetups last week, one about D3.js at Trulia HQ, one about GIS technologies and the Code for America HQ, and one about mapping at Stamen HQ. I picked all three because they are relevant to a geospatial data visualization that I am working on for my practicum at AutoGrid, but the last one is what I’m going to talk a bit about, because it was the most hands-on.  &lt;/p&gt;
&lt;p&gt;The workshop took place at Stamen Design’s headquarters in the Mission and was led by Eric Theise; you can see his beautiful/informative slides (created using &lt;a href="http://lab.hakim.se/reveal-js/#/"&gt;reveal.js&lt;/a&gt;) &lt;a href="http://erictheise.com/maptime_platform_slides/#/"&gt;here&lt;/a&gt;. Some useful Q&amp;amp;A happened on &lt;a href="http://www.meetup.com/Maptime-SF/events/147110652"&gt;the Meetup event page as well&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;This was actually a two-part workshop, but it was relatively painless to follow the instructions and get up to speed for the part II, so you should really give it a shot even now if it looks interesting.  &lt;/p&gt;
&lt;p&gt;First we got postgres up and running on our machines. I have local installations of MySQL, MongoDB, Hadoop and Hive up and running thanks to our course in Distributed Databases, but our class didn’t have time to get to postgres within a single credit hour. This, despite the fact that our professor admits to postgres being the best database to use if you have anything to say about it.  &lt;/p&gt;
&lt;p&gt;Next, we populated our database with some data from OpenStreetMap. Mike Migurski extracts data from OSM for major metropolitan areas on a semi-regular basis, so we used the San Francisco data &lt;a href="http://metro.teczno.com/#san-francisco"&gt;available on his web site&lt;/a&gt; via &lt;a href="http://wiki.openstreetmap.org/wiki/Osm2pgsql"&gt;osm2pgsql&lt;/a&gt;, a command-line utility that loads OpenStreetMap data into PostgreSQL databases.  &lt;/p&gt;
&lt;p&gt;Then we used &lt;a href="https://www.mapbox.com/tilemill/"&gt;TileMill&lt;/a&gt;, MapBox’s desktop application, to visualize our newborn database. We discovered how remarkably easy it can be to create vector layers for data contained in such a postGIS database using the same old SQL and CSS syntax you already know and love. Eric introduced us to some sensible pre-baked &lt;a href="https://github.com/gravitystorm/openstreetmap-carto"&gt;CartoCSS boilerplate&lt;/a&gt; courtesy of Andy Allen.  &lt;/p&gt;
&lt;p&gt;Lastly, we used a nifty feature of TileMill to actually bake our own map tiles and serve them up for use in our own maps. Note that if you want to do this, you’ll need the &lt;a href="https://github.com/mapbox/mbutil"&gt;mbutil command-line utility&lt;/a&gt;, not currently mentioned in the slide deck.  &lt;/p&gt;
&lt;p&gt;Not too shabby for 2 hours on a Wednesday night. Many thanks to the guys at Stamen for hosting, especially Eric for all his work on the slides. Not to mention the many other brilliant people who have made the tools and resources that allow something this involved and grandiose to be done on a laptop by someone who is still learning the ropes. Hands-on workshops like this are one of the best ways to learn these technologies. Case in point, I may not have ever stumbled across Mike or Andy’s resources had I not been learning directly from people who are intimately familiar with the practical ins and outs of digital cartography.  &lt;/p&gt;
&lt;p&gt;Digital mapping is rapidly capturing my interest because of the beautifully functional things one can do with it, and it seems like an amazing time to be learning it, because the ecosystem is beginning to really flourish. Looking forward to more events!  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Sun, 01 Dec 2013 02:37:00 -0800</pubDate><guid>tag:pelican.spencerboucher.com,2013-12-01:map-time-at-stamen/</guid><category>cartography</category><category>maps</category></item><item><title>StepLively update</title><link>http://pelican.spencerboucher.com/steplively-update/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;This week I got an email from RStudio letting me know that after a week or two of waiting, I’ve been assigned 2GB of space on their servers for hosting Shiny apps. So now you can click on the above link to see the live version of my stepwise regression app that I posted last week. In celebration, I’ve added a few extras to the app, including the ability to upload your own data set, although its not a particularly robust feature yet.  &lt;/p&gt;
&lt;p&gt;Update: I’ve recently moved to Rstudio’s next-gen hosting platform: shinyapps.io. This was necessary because I was having issues installing low-level packages like Rcpp onto their Spark server. This new platform is much more flexible in this regard. I’ve updated links accordingly.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Sun, 03 Nov 2013 21:59:00 -0800</pubDate><guid>tag:pelican.spencerboucher.com,2013-11-03:steplively-update/</guid><category>R</category><category>Shiny</category><category>stepwise</category></item><item><title>Switching R versions on-the-fly</title><link>http://pelican.spencerboucher.com/switching-r-versions-on-the-fly/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;If you need to switch back to an older version of R to use a particular package, it doesn’t get easier than downloading the &lt;a href="http://r.research.att.com/"&gt;RSwitcher utility from AT&amp;amp;T&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;I use homebrew to keep my main installation up-to-date, and then switching back to 2.7 when I need a certain package is as easy as one click.  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Tue, 22 Oct 2013 22:01:00 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2013-10-22:switching-r-versions-on-the-fly/</guid><category>R</category></item><item><title>Holo color palette</title><link>http://pelican.spencerboucher.com/holo-color-palette/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;Despite being slightly colorblind, I’m one of those
people who is bizarrely anal-retentive when it comes to color-schemes in
the software that I use. I use custom CSS extensions in Chrome to make
the color schemes of websites like Wikipedia, Youtube, etc a bit more
pleasing. I did some digging in the accessibility preference pane of
Adobe Reader to make the default document background a nice book-like
sepia tone which makes reading PDFs online a much less straining
endeavor (incidentally - Google Play Books &lt;em&gt;only&lt;/em&gt; has a white background
which is why I greatly prefer the Amazon Kindle web app). I’ve spent
wayy too long (as I think many geeks do), customizing my terminal and
custom android ROM themes. I think you get the picture.  &lt;/p&gt;
&lt;p&gt;Anyways, I had been using the Solarized palette for a lot of things
because it seems like its choices are a bit less arbitrary and has some
science behind it (although I haven’t done any research into how
legitimate that science is). Its a bit limiting, however, and recently
I’ve been attracted to the Holo color palette that ICS compliant Android
apps use. I couldn’t find a pre-compiled Holo palette in .clr format for
use in the Mac OSX Color Picker utility, so I made a simple, bare-bones
one. First I had to download the HEX plugin from &lt;a href="http://wafflesoftware.net/hexpicker/"&gt;wafflesoftware.net&lt;/a&gt;
which for some strange reason is not a functionality that Apple thought
worthwhile to put into a color picker (???). &lt;a href="http://db.tt/xU7lyIBh"&gt;You can download my .clr
palette right here&lt;/a&gt;. Maybe I can save the next anal-retentive googler
a few minutes.  &lt;/p&gt;
&lt;p&gt;PS. While I am talking about color - if you haven’t heard of &lt;a href="http://justgetflux.com/"&gt;fl.ux&lt;/a&gt;,
check it out. It is a magical little utility that warms the color of
your screen automatically after sunset and can &lt;em&gt;really&lt;/em&gt; save your eyes
and make getting to sleep a little bit easier if you are working in
front of your screen late at night.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Tue, 20 Aug 2013 06:19:00 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2013-08-20:holo-color-palette/</guid><category>color</category><category>holo</category><category>solarized</category></item><item><title>On being approximate</title><link>http://pelican.spencerboucher.com/on-being-approximate/</link><description>&lt;html&gt;&lt;body&gt;&lt;blockquote&gt;
&lt;p&gt;It is hard to tell how many investors beat the stock market&lt;/p&gt;
&lt;p&gt;over the long run,&lt;/p&gt;
&lt;p&gt;because the data is very noisy,&lt;/p&gt;
&lt;p&gt;but we know that most cannot&lt;/p&gt;
&lt;p&gt;relative to their level of risk,&lt;/p&gt;
&lt;p&gt;since trading produces no net excess return&lt;/p&gt;
&lt;p&gt;but entails transaction costs,&lt;/p&gt;
&lt;p&gt;so unless you have inside information&lt;/p&gt;
&lt;p&gt;you are probably better off investing in an index fund&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An inadvertent data poem by &lt;a href="http://www.usfca.edu/facultydetails.aspx?id=6442485442"&gt;Dr. Cindi Thompson&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Sat, 20 Jul 2013 05:39:00 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2013-07-20:on-being-approximate/</guid><category>approximation</category></item><item><title>Keeping It Real</title><link>http://pelican.spencerboucher.com/keeping-it-real/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;Today I was listening to a Freakonomics podcast about prediction and how monumentally terrible we are at it- even experts. The segment featured some small European country that had passed a law to fine or imprison psychics whose predictions were wrong. Why maintain such strict  standards for their predictions when we don’t hold anyone else similarly accountable. In all honesty I’m not sure I wouldn’t be in favor of such a law because such psychics are making the claim that they are delivering a service that they are demonstrably &lt;em&gt;not&lt;/em&gt; providing (i.e. &lt;em&gt;fraud&lt;/em&gt;), but that’s besides the point.  &lt;/p&gt;
&lt;p&gt;The point is that it got me thinking about accountability for predictions. They are everywhere, obviously. Pundits, politicians, financial speculators (oh my)… We have fact checking organizations that (to arguable degrees of success) provide a way of holding these people to their claims about the past and present. Obviously we couldn’t rightly hold these people’s predictions to the same standard that we do (or often don’t) when they are reciting facts, but &lt;strong&gt;&lt;em&gt;why the hell aren’t we keeping a track record&lt;/em&gt;&lt;/strong&gt;?  &lt;/p&gt;
&lt;p&gt;What if every time a president, congressman, political pundit, Fed chairman, or otherwise high-profile “expert” made a prediction, we entered it into a database? What if we actually kept statistics on how often there predictions panned out? Right now all the incentives are stacked to encourage wanton prediction-making, because we only keep track of the &lt;em&gt;hits&lt;/em&gt;, and not the &lt;em&gt;misses&lt;/em&gt;. Why? Because right now we are effectively letting the people who are making the predictions be the ones who keep track of their success by &lt;em&gt;not calling them out&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;If we switched the incentives around, its a game changer. First, experts become more careful with their predictions, improving their quality overall (because face it, right now they are almost always less than worthless). Second, we the public get an idea of whose predictions are actually worth listening to. I shudder (and then smile a bit) to think about how many careers would be dashed upon the rocks if the ability to produce this metric came about.  &lt;/p&gt;
&lt;p&gt;I picture a website - freely available to all - maybe with individual profiles for all the would-be Nostradomus. Maybe a staff of statisticians would have to keep track of all the predictions made, and users could suggest new profiles they would like to see added and monitored. Picture Mint.com, but predictions instead of transactions. For example, you could search Glenn Beck’s predictions by category, by time-frame, or by outcome (yes, no, partial?), and get awesome graphs and pie charts and a big ‘ole “percentage correct” next to your name.  &lt;/p&gt;
&lt;p&gt;Thoughts?  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Fri, 06 Jan 2012 05:11:00 -0800</pubDate><guid>tag:pelican.spencerboucher.com,2012-01-06:keeping-it-real/</guid><category>economics</category><category>freakonomics</category><category>politics</category><category>prediction</category><category>predictions</category></item><item><title>Thielfoundation.org</title><link>http://pelican.spencerboucher.com/thielfoundation-org/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;About a week ago, my house-mate invited me to has to have been one of the most interesting social events I’ve ever lucked into attending. The founder of Paypal, subsequent multimillion(billion?)aire, and venture capitalist Peter Thiel created a &lt;a href="http://www.thielfoundation.org/index.php?option=com_content&amp;amp;id=14:the-thiel-fellowship-20-under-20&amp;amp;catid=1&amp;amp;Itemid=16"&gt;foundation&lt;/a&gt; that offers fellowships that amount to anti-college scholarships. Under the program name "20 under 20", he provides brilliant, successful, creative teens with 100,000 dollars over the course of two years to NOT go to Ivy League schools and, instead, become entrepreneurs and and develop their ideas. They must have had some great applications because in their first year, the foundation couldn’t settle on 20 and so there’s something like 23 or 24 in actuality.  &lt;/p&gt;
&lt;p&gt;Incredible premise, you might say. And incredible it is.  &lt;/p&gt;
&lt;p&gt;My aforementioned house-mate (I haven’t decided if I am going to use real names in this blog yet) has somehow fallen in a few of the fellows, and let me tag along to a social get-together of the foundation. It took place right on the Bay in the Berkeley marina, surrounded by towering sailboats docked on shore.  &lt;/p&gt;
&lt;p&gt;We got there fairly late, but during the time that we were there we sure met some interesting characters. My housemate introduced me to a leader in the “seasteading” movement- which is an effort to develop offshore sovereign communities outside of any existing federal jurisdiction in an effort to establish and test alternative, highly libertarian societal structures. Or something like that. In addition, I was brought up to speed on the status of the first “charter cities” being created in Honduras. There is a fantastic TED talk concerning these charter cities that I highly recommend.  &lt;/p&gt;
&lt;p&gt;That conversation led us into a discussion with a gentleman about the possibilities such a city would present for medical tourism. Eventually it was discovered that he works for a cryogenics biotech company, and actually used to be the president of Alcor Life Extension Foundation. I’ve been interested in Alcor for a while because its such a controversial and fascinating idea- they will chemically and cryogenically preserve your body (or just your head- if you are on a budget) after death so that if and when we develop technologies to reinstate neural activity, you’ll still be there to do so. This of course raises a whole host of interesting philosophical issues, like what kind of role continuity of neural activity plays in the issue of consciousness or identity, if any. Maybe another blog post on that some day soon.  &lt;/p&gt;
&lt;p&gt;Later I met someone working on developing models of brain function in the human neocortex. One of their key premises is that we can create these models in any way we want as long as we successfully mimic the output of the brain, but I happen to disagree on this point. Modeling true intelligence is more than just mimicking output, the &lt;em&gt;way&lt;/em&gt; in which computation is performed is nontrivial. Apparently, another company in the Bay area is taking exactly this approach. Numenta was founded by Jeff Hawkins, the guy who invented Palm Pilot, and is developing cortical learning algorithms that are based on actual biology- much more meaningful in my opinion. I’m in the process of reading everything about Numenta’s work at the moment, and I highly recommend Jeff Hawkins’ TED talk as well if you are at all interested in AI, the brain, or intelligence. There will definitely be another blog post in the near future concerning this.  &lt;/p&gt;
&lt;p&gt;Oh, and we almost got abducted by a taco truck. We were both starving and heard the food truck was about to leave, so we ran out to it, but it looked closed up. Several people were climbing in the side door however, so we followed them in hunger-fueled desperation, only to find the door closed behind us. Apparently these other people just needed a ride back to San Fran and were hitchhiking on the truck, but fortunately we managed to bail out before we wound up as unwitting taco-making indentured servants.  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Mon, 26 Sep 2011 03:58:00 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2011-09-26:thielfoundation-org/</guid><category>entrepeneurship</category><category>silicon valley</category></item><item><title>Nonlinear</title><link>http://pelican.spencerboucher.com/nonlinear/</link><description>&lt;html&gt;&lt;body&gt;&lt;p&gt;If there is one thing I have learned since graduating from college, it is that life is not linear. I feel that high school, college, and social constructions I have grown up in had programmed me to conceive of life in a very stepwise, linear way. Maybe it’s because you are working so single-mindedly toward holding that diploma in your hand. Or perhaps because you just don’t have much exposure beyond the people who are doing mostly what you are doing. I could be wrong, but I think a lot of college kids have this mentality.&lt;/p&gt;
&lt;p&gt;But it just isn’t. Or at least it doesn’t have to be if you don’t want it to be. You can do absolutely anything you want to in this world, and the arena is HUGE. Like, unimaginably huge. And the Bay Area has to be one of the most incredible places in the world to make this fact hit home. My new home is a mecca for so many different cultural movements. Silicon Valley, entrepeneurs, artists, hippies, LGBT communities, futurists, libertarians, and of course scientists, the last of which I’d like to think I fall into.&lt;/p&gt;
&lt;p&gt;My favorite thing about moving here by far is simply how &lt;em&gt;interesting&lt;/em&gt; everyone I meet is. I mean really really fascinating. Maybe I’ve just been somehow missing how intrinsically awesome the random people I would meet in Tennessee and Houston were. But here, in a single night you can meet people who work for Google and Facebook, people who hunt planets for NASA, people crafting all manner of crazy or thoughtful or brilliant startups, people who have attended universities all over the country and the world… I could go on and on. &lt;/p&gt;
&lt;p&gt;That’s one thing I hope to do with this blog: highlight the amazing people and projects that I come across. These people are not living linear lives. And just being in this crazy, fast paced, marvelously jumbled community is creating a paradigm shift in the way that I see the world. Its tough to explain, but I guess I’m realizing how things are out there for you to take and affect and become a part of- &lt;em&gt;big&lt;/em&gt; things- and all you have to do is try. The rough plan for this blog is to make it about &lt;strong&gt;ideas&lt;/strong&gt;, &lt;strong&gt;adventures&lt;/strong&gt;, and &lt;strong&gt;discoveries&lt;/strong&gt;. No promises for how it will turn out developing though.&lt;/p&gt;
&lt;p&gt;Here’s to being nonlinear!&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Spencer Boucher</dc:creator><pubDate>Sun, 25 Sep 2011 00:59:00 -0700</pubDate><guid>tag:pelican.spencerboucher.com,2011-09-25:nonlinear/</guid></item></channel></rss>