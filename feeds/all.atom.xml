<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>justmytwospence</title><link href="http://justmytwospence.github.com/pelican/" rel="alternate"></link><link href="http://justmytwospence.github.com/pelican/feeds/all.atom.xml" rel="self"></link><id>http://justmytwospence.github.com/pelican/</id><updated>2014-03-22T18:38:32+01:00</updated><entry><title>Stratified sampling in R</title><link href="http://justmytwospence.github.com/pelican/stratified-sampling-in-r.html" rel="alternate"></link><updated>2014-03-22T18:38:32+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2014-03-22:stratified-sampling-in-r.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;I was surprised to find that R doesn’t have a base function for stratified random sampling. There’s not even a well known package I could find that does this in a straight forward way. So heres my own.&lt;/p&gt;
&lt;p&gt;It is essentially a wrapper for a ddply call that samples each subset and then combines them. If the size argument is less than 1, it will be interpreted as the percentage of each stratification subset that should be sampled. If the size argument is greater than 1, it will be interpreted as the number of observations to sample from each stratification subset. &lt;/p&gt;
&lt;p&gt;Note that in the first case, a different number of observations will be taken from each subset depending on their total number of observations. In the second case however, an equal number of observations will be sampled from each subset, regardless of their total number of observations.&lt;/p&gt;
&lt;p&gt;The .by argument is formulated the same way it is for any other ddply call.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;script src="https://gist.github.com/justmytwospence/7937389.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;</summary></entry><entry><title>Test of code blocks</title><link href="http://justmytwospence.github.com/pelican/test-of-code-blocks.html" rel="alternate"></link><updated>2014-03-21T10:20:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2014-03-21:test-of-code-blocks.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;This is the content of my super blog post.&lt;/p&gt;
&lt;p&gt;Testing Python code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mp&lt;/span&gt;
&lt;span class="n"&gt;pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;boop&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;boops&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;boop&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Testing R code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;library&lt;span class="p"&gt;(&lt;/span&gt;randomForest&lt;span class="p"&gt;)&lt;/span&gt;
d &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'file.csv'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
ggplot&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;=&lt;/span&gt;d&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    geom_point&lt;span class="p"&gt;(&lt;/span&gt;aes&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="o"&gt;=&lt;/span&gt;x&lt;span class="p"&gt;,&lt;/span&gt; y&lt;span class="o"&gt;=&lt;/span&gt;y&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Testing shell code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_PRIVATE_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"$(/bin/ls "&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;"/.ec2/pk-*.pem | /usr/bin/head -1)"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</summary></entry><entry><title>What’s a Mooc?</title><link href="http://justmytwospence.github.com/pelican/whats-a-mooc.html" rel="alternate"></link><updated>2014-03-14T14:13:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2014-03-14:whats-a-mooc.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;Turns out that Robert de Niro and friends are facing a problem very  similar to the one facing Coursera and friends…&lt;/p&gt;
&lt;div align="center" class="youtube"&gt;&lt;iframe frameborder="0" height="315" src="https://www.youtube.com/embed/8vw8t4O9JQM" width="420"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;p&gt;From the 1973 movie &lt;a class="reference external" href="http://www.imdb.com/title/tt0070379/?ref_=fn_al_tt_1"&gt;Mean Streets&lt;/a&gt;. Apparently Scorsese predicted the education revolution wayy ahead of his time and had a pretty good grasp on the biggest challenges that MOOCs would need to overcome.&lt;/p&gt;
&lt;/body&gt;&lt;/html&gt;</summary><category term="Coursera"></category><category term="education"></category><category term="MOOC"></category></entry><entry><title>writeLaTeX</title><link href="http://justmytwospence.github.com/pelican/writelatex.html" rel="alternate"></link><updated>2014-01-25T18:42:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2014-01-25:writelatex.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;If you use \(\LaTeX\) and haven’t yet heard of
\(write\LaTeX\), do yourself a favor and check it out. On one level
its a really great version of Google Docs for documents that are
properly typeset, which is incredibly useful because a huge swathe of
the documents created with \(\LaTeX\) are inherently collaborative in
nature. Its other fantastic feature is that your \(\LaTeX\) markup is
automatically rendered in real-time (or close to it, at least, there’s a
few seconds of lag depending on the length of the document). This made
writing my first \(\LaTeX\) intensive document a great experience,
because I could experiment liberally with equations and figure
placement. There’s literally zero barrier to entry because its
completely web based; you don’t need to install a thing and templates
are available to get you jump-started. Right now other engines like
XeLaTeX aren’t supported, but I believe they are in the works.&lt;/p&gt;
&lt;p&gt;So every excuse you ever had to not learn \(\LaTeX\) has been
obliterated. Give [\(write\LaTeX\)][] a try, or be passive aggressive
and send it to that collaborator that always sends you everything in a
poorly formatted Word document. I myself will be experimenting with
using \(\LaTeX\) to take math-heavy notes in real-time, which sounds
crazy but the live rendering makes the attempt feasible.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="latex"></category><category term="tex"></category><category term="typesetting"></category><category term="writelatex"></category></entry><entry><title>Scheduling tasks in the cloud with EC2 APIs</title><link href="http://justmytwospence.github.com/pelican/ec2-apis.html" rel="alternate"></link><updated>2014-01-12T17:53:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2014-01-12:ec2-apis.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;This post is sort of an addendum to our &lt;a href="http://www.spencerboucher.com/live-mapping/" title="Live mapping"&gt;live-mapping project&lt;/a&gt;, but it should also be of use to anyone looking to run an arbitrary script on a recurring schedule. Originally, we set up a 24/7 instance on &lt;a href="http://aws.amazon.com/ec2/"&gt;Amazon’s Elastic Compute Cloud&lt;/a&gt; that ran a daily &lt;code&gt;cron&lt;/code&gt; job. This works, but its a bit wasteful because we’re paying for 24 hours of cloud even though we’re only actually using it for maybe 5 minutes a day.
&lt;br/&gt;
Fortunately, Amazon provides a &lt;a href="http://aws.amazon.com/developertools/"&gt;schmorgesborg&lt;/a&gt; of command line interface (&lt;span class="caps"&gt;CLI&lt;/span&gt;) tools that allow us to manage our cloud instances more efficiently. Specifically, we want to schedule an instance to spin up only once a day, execute our script, then shut back down. To accomplish this, we will want three &lt;span class="caps"&gt;CLI&lt;/span&gt; tools: &lt;a href="http://aws.amazon.com/developertools/368"&gt;the Amazon &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;AMI&lt;/span&gt; Tools&lt;/a&gt;, &lt;a href="http://aws.amazon.com/developertools/351"&gt;the Amazon &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; Tools&lt;/a&gt;,and &lt;a href="http://aws.amazon.com/developertools/2535"&gt;the Auto Scaling Command Line Tool&lt;/a&gt;. If you’re on a Mac, it’s way easier to get these with &lt;a href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; than by downloading from Amazon’s website: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;brew install ec2-ami-tools &lt;span class="c"&gt;# For creating an AMI from an existing machine&lt;/span&gt;
brew install ec2-api-tools &lt;span class="c"&gt;# For registering and launching instances&lt;/span&gt;
brew install aws-as        &lt;span class="c"&gt;# For creating auto scaling groups / defining schedules&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As an extra Homebrew bonus, running &lt;code&gt;brew info ec2-ami-tools&lt;/code&gt;, &lt;code&gt;brew info ec2-api-tools&lt;/code&gt;, and &lt;code&gt;brew info aws-as&lt;/code&gt; will now tell us exactly what we need to do to get our authentication and environment variables all set up. First we are told to download the necessary .pem files from &lt;a href="http://aws-portal.amazon.com/gp/aws/developer/account/index.html?action=access-key"&gt;this Amazon page&lt;/a&gt; and place them into a new hidden directory of our home directory &lt;code&gt;.ec2&lt;/code&gt;. Then we tell our command line where everything lives now by inserting the following lines into our &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_PRIVATE_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"$(/bin/ls "&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;"/.ec2/pk-*.pem | /usr/bin/head -1)"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_CERT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"$(/bin/ls "&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;"/.ec2/cert-*.pem | /usr/bin/head -1)"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/usr/local/Cellar/ec2-api-tools/1.6.12.0/libexec"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;AWS_AUTO_SCALING_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/usr/local/Cellar/auto-scaling/1.0.61.4/libexec"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_AMITOOL_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/usr/local/Cellar/ec2-ami-tools/1.4.0.9/libexec"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_REGION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"us-west-2"&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_ZONE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EC2_REGION&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;a
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;EC2_URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;https://&lt;span class="nv"&gt;$EC2_REGION&lt;/span&gt;.ec2.amazonaws.com
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;AWS_AUTO_SCALING_URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;https://autoscaling.&lt;span class="nv"&gt;$EC2_REGION&lt;/span&gt;.amazonaws.com
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Its pretty simple, but if you have any trouble with this part, refer to the official &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/SettingUp_CommandLine.html"&gt;Amazon documentation for setting up the command line&lt;/a&gt;.
&lt;br/&gt;
Because these environment variables are recognized out of the box by the &lt;span class="caps"&gt;CLI&lt;/span&gt; tools, we won’t need to point to our authentication keys or specify a region every time we make an &lt;span class="caps"&gt;API&lt;/span&gt; call and our next commands will be much more succinct. Note that every &lt;span class="caps"&gt;EC2&lt;/span&gt; instance is physically located at one of several regions; we are using us-west-2 because it happens to be where I spun up the existing instance that currently holds our “update.py” script, but any of them would probably work just fine for the simple job at hand.
&lt;br/&gt;
&lt;/p&gt;&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;th&gt;Region&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ap-northeast-1&lt;/td&gt;
&lt;td&gt; Asia Pacific (Tokyo) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ap-southeast-1&lt;/td&gt;
&lt;td&gt;Asia Pacific (Singapore) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ap-southeast-2&lt;/td&gt;
&lt;td&gt;Asia Pacific (Sydney) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;eu-west-1&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;EU&lt;/span&gt; (Ireland) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sa-east-1&lt;/td&gt;
&lt;td&gt;South America (Sao Paulo) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;us-east-1&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;US&lt;/span&gt; East (Northern Virginia) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;us-west-1&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;US&lt;/span&gt; West (Northern California) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;us-west-2&lt;/td&gt;
&lt;td&gt;&lt;span class="caps"&gt;US&lt;/span&gt; West (Oregon) Region&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;br/&gt;
So, first things first. We can’t just spin up an off-the-rack &lt;span class="caps"&gt;EC2&lt;/span&gt; instance every day, because we’ll run into the same problem that I originally had with my web host: the Python modules that we need won’t be installed. We &lt;em&gt;could&lt;/em&gt; write a script that would install &lt;code&gt;pip&lt;/code&gt; plus all of the requisite Python modules and run it first thing after we launch the instance, but there’s a better way:
&lt;div class="highlight"&gt;&lt;pre&gt;ec2-create-image i-8918e1be -n &lt;span class="s2"&gt;"Map Update Image"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This command from &lt;code&gt;ec2-ami-tools&lt;/code&gt; creates an “Amazon Machine Image” of the instance that we previously had running and names it “Map Update Image”. A new image &lt;span class="caps"&gt;ID&lt;/span&gt; will now print to your console, &lt;code&gt;ami-fcdfb9cc&lt;/code&gt; in my case. This is tantamount to cloning the instance, because we can now reference the new image &lt;span class="caps"&gt;ID&lt;/span&gt; when we spin up new instances and all of our modules, scripts, etc. will be there waiting for us. Note that I removed the instance’s &lt;code&gt;cron&lt;/code&gt; job &lt;em&gt;before&lt;/em&gt; creating the &lt;span class="caps"&gt;AMI&lt;/span&gt;, because we’ll now be handling the task scheduling from &lt;em&gt;outside&lt;/em&gt; the instance, via &lt;strong&gt;autoscaling&lt;/strong&gt;.
&lt;br/&gt;
Next let’s write a shell script that will execute our Python map-updating script, shoot us a diagnostic email, then shut down the instance that its running on. The idea here is that once a day we’re going to spin up an instance using our shiny new &lt;span class="caps"&gt;AMI&lt;/span&gt; and immediately run this new script (let’s call it “update.sh”) that will do its business and then promptly commit seppuku and stop charging us money. Eric Hammond has created a great template on &lt;a href="http://alestic.com/2011/11/ec2-schedule-instance"&gt;his blog&lt;/a&gt;, which I’ve modified below. Note the execution of our &lt;a href="http://www.spencerboucher.com/live-mapping/" title="Live mapping"&gt;familiar&lt;/a&gt; “update.py” script highlighted on line 4, and the apoptosis command on line 46:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash -x&lt;/span&gt;
&lt;span class="nb"&gt;exec&lt;/span&gt; &amp;gt; &amp;gt;&lt;span class="o"&gt;(&lt;/span&gt;tee /var/log/user-data.log|logger -t user-data -s 2&amp;gt;/dev/console&lt;span class="o"&gt;)&lt;/span&gt; 2&amp;gt;&amp;amp;1

/usr/bin/python /home/ubuntu/update.py &lt;span class="c"&gt;# Run the script&lt;/span&gt;

&lt;span class="nv"&gt;EMAIL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;spencer.g.boucher@gmail.com

&lt;span class="c"&gt;# Upgrade and install Postfix so we can send a sample email&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;DEBIAN_FRONTEND&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;noninteractive
apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get upgrade -y &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y postfix

&lt;span class="c"&gt;# Get some information about the running instance&lt;/span&gt;
&lt;span class="nv"&gt;instance_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;wget -qO- instance-data/latest/meta-data/instance-id&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;public_ip&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;wget -qO- instance-data/latest/meta-data/public-ipv4&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;zone&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;wget -qO- instance-data/latest/meta-data/placement/availability-zone&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;region&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;expr match &lt;span class="nv"&gt;$zone&lt;/span&gt; &lt;span class="s1"&gt;'\(.*\).'&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;uptime&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;uptime&lt;span class="k"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Send status email&lt;/span&gt;
/usr/sbin/sendmail -oi -t -f &lt;span class="nv"&gt;$EMAIL&lt;/span&gt; &lt;span class="s"&gt;&amp;lt;&amp;lt;EOM&lt;/span&gt;
&lt;span class="s"&gt;From: $EMAIL&lt;/span&gt;
&lt;span class="s"&gt;To: $EMAIL&lt;/span&gt;
&lt;span class="s"&gt;Subject: Results of EC2 scheduled script&lt;/span&gt;

&lt;span class="s"&gt;This email message was generated on the following EC2 instance:&lt;/span&gt;

&lt;span class="s"&gt;  instance id: $instance_id&lt;/span&gt;
&lt;span class="s"&gt;  region:      $region&lt;/span&gt;
&lt;span class="s"&gt;  public ip:   $public_ip&lt;/span&gt;
&lt;span class="s"&gt;  uptime:      $uptime&lt;/span&gt;

&lt;span class="s"&gt;If the instance is still running, you can monitor the output of this&lt;/span&gt;
&lt;span class="s"&gt;job using a command like:&lt;/span&gt;

&lt;span class="s"&gt;  ssh ubuntu@$public_ip tail -1000f /var/log/user-data.log&lt;/span&gt;

&lt;span class="s"&gt;  ec2-describe-instances --region $region $instance_id&lt;/span&gt;

&lt;span class="s"&gt;EOM&lt;/span&gt;

&lt;span class="c"&gt;# Give the script and email some time to do their thing&lt;/span&gt;
sleep 600 &lt;span class="c"&gt;# 10 minutes&lt;/span&gt;

&lt;span class="c"&gt;# This will stop the EBS boot instance, stopping the hourly charges.&lt;/span&gt;
&lt;span class="c"&gt;# Have Auto Scaling terminate it, stopping the storage charges.&lt;/span&gt;
shutdown -h now

&lt;span class="nb"&gt;exit &lt;/span&gt;0
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the user data script that we pass to the launch configuration executes with &lt;em&gt;root&lt;/em&gt; permissions, not as the user “ubuntu” that you would typically log in as via &lt;code&gt;ssh&lt;/code&gt;. Its probably best to be as explicit as possible when specifying path names in the cloud, the tilde operator might turn around and bite you.
&lt;br/&gt;
Now we need to create &lt;strong&gt;launch configuration&lt;/strong&gt; that will basically do all the button-pushing that we would normally be doing at the &lt;span class="caps"&gt;AWS&lt;/span&gt; console &lt;span class="caps"&gt;GUI&lt;/span&gt;. 
&lt;br/&gt;
Here we specify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="dquo"&gt;“&lt;/span&gt;Micro” as our instance type.&lt;/li&gt;
&lt;li&gt;Our shell script “update.sh” from step 2 as the “user-data-file”. User data files are passed into the instance and executed immediately when supplied in the launch configuration. They must be less than 16kb as I suppose they are stored on some ancillary server somewhere.&lt;/li&gt;
&lt;li&gt;The &lt;span class="caps"&gt;AMI&lt;/span&gt; image that we cloned in step 1 from the instance that included our Python modules.&lt;/li&gt;
&lt;li&gt;The name of the launch config; let’s call it “map-update-launch-config”.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;as-create-launch-config
   --instance-type t1.micro
   --user-data-file ~/Desktop/update.sh
   --image-id ami-fcdfb9cc
   --launch-config &lt;span class="s2"&gt;"map-update-launch-config"&lt;/span&gt;
as-describe-launch-configs --headers
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the second line provides a list of all the launch configurations that have been created.
&lt;br/&gt;
We must also create an &lt;strong&gt;auto scaling group&lt;/strong&gt;. These are typically used as a sort of container to which we can add/remove instances on a schedule or in response to heavy traffic, but we can also use it to schedule a single instance to flick on and off. We need to tell it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A name to assign the scaling group (“map-update-scale-group”).&lt;/li&gt;
&lt;li&gt;The name of the launch configuration we created in step 3 (“map-update-launch-config”).&lt;/li&gt;
&lt;li&gt;Which availability zone we want to use (basically irrelevant; we set our environment variable &lt;code&gt;EC2_ZONE&lt;/code&gt; to “a” earlier). &lt;code&gt;ec2-describe-available-zones&lt;/code&gt; provides a list of the available zones&lt;/li&gt;
&lt;li&gt;A minimum and maximum number of instances in the group. We’ll initialize these to zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Create auto scaling group with launch configuration"&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
as-create-auto-scaling-group
   --auto-scaling-group &lt;span class="s2"&gt;"map-update-scale-group"&lt;/span&gt;
   --launch-configuration &lt;span class="s2"&gt;"map-update-launch-config"&lt;/span&gt;
   --availability-zones &lt;span class="s2"&gt;"$EC2_ZONE"&lt;/span&gt;
   --min-size 0 --max-size 0
as-suspend-processes &lt;span class="s2"&gt;"map-update-scale-group"&lt;/span&gt;
   --processes ReplaceUnhealthy
as-describe-auto-scaling-groups --headers
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the second line, we are using &lt;code&gt;as-suspend-processes&lt;/code&gt; to prevent the instance’s default behavior which is to attempt to restart after it is shut down. The third line provides a list of all the auto scaling groups that have been created.
&lt;br/&gt;
Last but not least, we are ready to assign a schedule to our auto scaling group. Here we create two: one to start the instance and one to terminate the instance. Astute readers will recall that “update.sh” already &lt;em&gt;stops&lt;/em&gt; the instance so that we aren’t paying to have it running, but we also need to completely &lt;em&gt;terminate&lt;/em&gt; the instance so that we aren’t paying to store information about it. Each schedule requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A name (“map-update-start” &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; “map-update-stop”).&lt;/li&gt;
&lt;li&gt;The name of the auto scaling group we created in step 4 (“map-update-scale-group”).&lt;/li&gt;
&lt;li&gt;How we want to scale. By setting both &lt;code&gt;min-size&lt;/code&gt; and &lt;code&gt;max-size&lt;/code&gt; to 1, we are effectively turning on one instance. We later effectively turn that instance back off by setting both to 0.&lt;/li&gt;
&lt;li&gt;A “recurrence,” ie when to occur. This flag uses the same syntax that &lt;code&gt;cron&lt;/code&gt; does. Here we set the instance to launch at midnight &lt;span class="caps"&gt;UTC&lt;/span&gt; (&lt;code&gt;0 0 * * *&lt;/code&gt;), and terminate 15 minutes later (&lt;code&gt;15 0 * * *&lt;/code&gt;). Recall that our script already stops the instance 10 minutes after execution, so 15 minutes is playing it safe.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;as-put-scheduled-update-group-action
   --name &lt;span class="s2"&gt;"map-update-start"&lt;/span&gt;
   --auto-scaling-group &lt;span class="s2"&gt;"map-update-scale-group"&lt;/span&gt;
   --min-size 1 --max-size 1
   --recurrence &lt;span class="s2"&gt;"0 0 * * *"&lt;/span&gt;
as-put-scheduled-update-group-action
   --name &lt;span class="s2"&gt;"map-update-stop"&lt;/span&gt;
   --auto-scaling-group &lt;span class="s2"&gt;"map-update-scale-group"&lt;/span&gt;
   --min-size 0 --max-size 0
   --recurrence &lt;span class="s2"&gt;"15 0 * * *"&lt;/span&gt;
as-describe-scheduled-actions --headers
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As before, the third line provides a list of the actions that have been scheduled.
&lt;br/&gt;
And thats it! We are now only paying for 10 or 15 minutes of cloud per day, as opposed to 1,440 of them. To review the timeline we have created in this example: our auto scaling group boots up an instance up at midnight &lt;span class="caps"&gt;UTC&lt;/span&gt; that immediately executes “update.sh”. This automatically executes “update.py” and shoots us a diagnostic email. It then waits 10 minutes to make sure everything has time to run, before stopping the instance. 5 minutes after &lt;em&gt;that&lt;/em&gt; the auto scaling group then completely terminates the instance.
&lt;br/&gt;
Other great resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/schedule_time.html"&gt;Official Amazon documentation for scheduling auto scaling groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://alestic.com/2011/11/ec2-schedule-instance"&gt;Running &lt;span class="caps"&gt;EC2&lt;/span&gt; Instances on a Recurring Schedule with Auto Scaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robertsindall.co.uk/blog/how-to-use-amazons-auto-scaling-groups/"&gt;Summary of &lt;span class="caps"&gt;API&lt;/span&gt; commands&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cardinalpath.com/autoscaling-your-website-with-amazon-web-services-part-2/"&gt;Auto Scaling Your Website with Amazon Web Services&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="api"></category><category term="auto scaling"></category><category term="aws"></category><category term="cloud"></category><category term="cron"></category><category term="ec2"></category></entry><entry><title>Titanic: Getting Started With R</title><link href="http://justmytwospence.github.com/pelican/titanic-getting-started-with-r.html" rel="alternate"></link><updated>2014-01-11T22:05:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2014-01-11:titanic-getting-started-with-r.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;My friend and classmate &lt;a href="http://trevorstephens.com"&gt;Trevor Stephens&lt;/a&gt; has created some &lt;a href="http://trevorstephens.com/post/72916401642/titanic-getting-started-with-r"&gt;pretty stellar R tutorials&lt;/a&gt; that will take you to about halfway up the leaderboard of Kaggle’s &lt;a href="http://www.kaggle.com/c/titanic-gettingStarted"&gt;Titanic: Machine Learning from Disaster&lt;/a&gt; competition. While the competition has a &lt;a href="http://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-python"&gt;Python tutorial&lt;/a&gt; and even a beginner’s &lt;a href="http://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-excel"&gt;Excel tutorial&lt;/a&gt;, any R equivalent had been suspiciously lacking. This is the competition that served as our first foray into machine learning, so kudos to Trevor for giving back to the community!&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="kaggle"></category><category term="machine learning"></category><category term="R"></category></entry><entry><title>Live mapping</title><link href="http://justmytwospence.github.com/pelican/live-mapping.html" rel="alternate"></link><updated>2014-01-08T08:50:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2014-01-08:live-mapping.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;I’ve been wanting to do some more mapping stuff since my first encounter with Leaflet a month or two ago while I was working on a project for &lt;a href="http://auto-grid.com"&gt;AutoGrid&lt;/a&gt;. I had my eye on CartoDB’s time series library, &lt;a href="https://github.com/cartodb/torque"&gt;Torque&lt;/a&gt;, because I had really wanted to do some time-series visualization, but time constraints and privacy issues with uploading data to CartoDB’s servers prevented me from really exploring. Since I had a few days of free time over winter break, I played around with it a bit and came up with this: &lt;a href="http://www.spencerboucher.com/map"&gt;spencerboucher.com/map&lt;/a&gt;. How’d I do it?
&lt;br/&gt;
First I needed some geographic data, so I turned to a source of data I’ve been collected for almost a year - my own location. &lt;a href="http://openpaths.cc"&gt;OpenPaths&lt;/a&gt; is a mobile app that records your location at regular time intervals. I opted for every 30 minutes at first, then upped it to every 15 minutes when I discovered that the effect on battery life wasn’t nearly as bad as I expected it to be. OpenPaths is a project of &lt;a href="http://nytlabs.com/"&gt;the R&amp;amp;D department at The New York Times&lt;/a&gt; and they &lt;a href="https://openpaths.cc/FAQ"&gt;claim&lt;/a&gt; that you are the only one with access to the collected data. Interestingly, you can grant various &lt;a href="https://openpaths.cc/projects"&gt;research programs&lt;/a&gt; access to your data at your own discretion. Your data is conveniently downloadable as a csv, json, or kml file, so I easily pulled my dataset of \~3,000 time points since December 2012. Unfortunately, I made the switch from iPhone to Android around April (well, that part is fortunate), and forgot to re-download the app, so I only really have data from the around the first three months and last two months of 2013.
&lt;br/&gt;
Turns out, making impressive maps with CartoDB is almost embarrassingly easy. Their &lt;span class="caps"&gt;GUI&lt;/span&gt; is pretty intuitive and running queries on their postgreSQL database is simple. Even time series stuff built on the Torque backend is really just point and click. I decided that the best way to visualize this data was with an aggregated hexbin heatmap of all my past locations, overlaid with a point-by-point replay with a time-slider. From there, it was just a one-line &lt;span class="caps"&gt;API&lt;/span&gt; call to host the map on my website (line 30 highlighted below), which is significantly easier than the legwork that went into crafting a Leaflet map “manually.”&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;html&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;head&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;meta&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;"viewport"&lt;/span&gt; &lt;span class="na"&gt;content=&lt;/span&gt;&lt;span class="s"&gt;"initial-scale=1.0, user-scalable=no"&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;title&amp;gt;&lt;/span&gt;Location | Spencer&lt;span class="nt"&gt;&amp;lt;/title&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;link&lt;/span&gt; &lt;span class="na"&gt;rel=&lt;/span&gt;&lt;span class="s"&gt;"shortcut icon"&lt;/span&gt; &lt;span class="na"&gt;href=&lt;/span&gt;&lt;span class="s"&gt;"http://spencerboucher.com/map/favicon.png"&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;link&lt;/span&gt; &lt;span class="na"&gt;rel=&lt;/span&gt;&lt;span class="s"&gt;"stylesheet"&lt;/span&gt; &lt;span class="na"&gt;href=&lt;/span&gt;&lt;span class="s"&gt;"http://libs.cartocdn.com/cartodb.js/v3/themes/css/cartodb.css"&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="c"&gt;&amp;lt;!--[if lte IE 8]&amp;gt;&lt;/span&gt;
&lt;span class="c"&gt;    &amp;lt;link rel="stylesheet" href="http://libs.cartocdn.com/cartodb.js/v3/themes/css/cartodb.ie.css" /&amp;gt;&lt;/span&gt;
&lt;span class="c"&gt;  &amp;lt;![endif]--&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;style &lt;/span&gt;&lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;"text/css"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;html&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;body&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;#map&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="k"&gt;margin&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
          &lt;span class="k"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
          &lt;span class="k"&gt;width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
          &lt;span class="k"&gt;height&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
          &lt;span class="k"&gt;background&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;black&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="nf"&gt;#cartodb-gmaps-attribution&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;visibility&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="k"&gt;hidden&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/style&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;script &lt;/span&gt;&lt;span class="na"&gt;src=&lt;/span&gt;&lt;span class="s"&gt;"http://maps.google.com/maps/api/js?v=3.2&amp;amp;sensor=false"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;script &lt;/span&gt;&lt;span class="na"&gt;src=&lt;/span&gt;&lt;span class="s"&gt;"http://libs.cartocdn.com/cartodb.js/v3/cartodb.js"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
    &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
      &lt;span class="nx"&gt;cartodb&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;createVis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'map'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'http://justmytwospence.cartodb.com/api/v2/viz/e8fd87d0-78b3-11e3-a9e9-e7941b6e2df0/viz.json'&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;body&lt;/span&gt; &lt;span class="na"&gt;onload=&lt;/span&gt;&lt;span class="s"&gt;"init()"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;id=&lt;/span&gt;&lt;span class="s"&gt;'map'&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is pretty awesome, but in light of how easy it all was, I was almost disappointed. Can we take it one step further? Let’s put on our &lt;a href="http://quantifiedself.com/about/"&gt;Quantified Self&lt;/a&gt; hats and set about to make this map &lt;em&gt;live&lt;/em&gt;. There’s three components to making this happen, so we’ll step through them one at a time. First we need to access the most recent data from OpenPaths (there’s an &lt;span class="caps"&gt;API&lt;/span&gt; for that!), and then we need to insert that data into CartoDB’s database (guess what, there’s an &lt;span class="caps"&gt;API&lt;/span&gt; for that too). Last but not least, we need to schedule that data transplant to occur on a regular basis. The Unix utility &lt;code&gt;cron&lt;/code&gt; is the canonical tool for this type of thing, so this seemed like a good time to learn how to use it.

Python has a reputation for being a great “glue” language, so that’s what we’ll use to build this script.
&lt;br/&gt;
Programmatically accessing your data from OpenPaths is super simple. This piece of our script is pulled more or less verbatim from &lt;a href="https://openpaths.cc/api"&gt;the OpenPaths &lt;span class="caps"&gt;API&lt;/span&gt; documentation&lt;/a&gt;. Line 21 (highlighted below) is key - this is where we specify which data you want to pull for injection into the CartoDB database. Here we will grab the last 24 hours of data (\~96 readings, if you’re collecting every 15 minutes like me), getting the results in a nice  &lt;span class="caps"&gt;JSON&lt;/span&gt;-formatted variable named &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;urllib&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="n"&gt;ACCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'redacted'&lt;/span&gt;
&lt;span class="n"&gt;SECRET&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'redacted'&lt;/span&gt;
&lt;span class="n"&gt;URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'https://openpaths.cc/api/1'&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build_auth_header&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s"&gt;'oauth_version'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"1.0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;'oauth_nonce'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate_nonce&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="s"&gt;'oauth_timestamp'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;consumer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Consumer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ACCESS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;secret&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SECRET&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'oauth_consumer_key'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;consumer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; 
    &lt;span class="n"&gt;request&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;signature_method&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;oauth2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SignatureMethod_HMAC_SHA1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign_request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;signature_method&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;consumer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_header&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;'start_time'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'end_time'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c"&gt;# get the last 24 hours&lt;/span&gt;
&lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;?&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;URL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;urllib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;urlencode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c"&gt;#print(query)&lt;/span&gt;
&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;request&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;build_auth_header&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;URL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'GET'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;readlines&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HTTPError&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we need to get our new &lt;code&gt;data&lt;/code&gt; variable into CartoDB’s postgreSQL server. &lt;a href="http://developers.cartodb.com/documentation/sql-api.html"&gt;CartoDB’s &lt;span class="caps"&gt;SQL&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; documentation&lt;/a&gt; makes this possible, and there’s even a &lt;a href="https://github.com/vizzuality/cartodb-python"&gt;python module&lt;/a&gt; that wraps OAuth2 to simplify things. Although its still in the early stages of development, this module works fine for our current purposes; all we have to do is send it a string that holds the &lt;span class="caps"&gt;SQL&lt;/span&gt; query we want to run. So now we’ll just write a for-loop that successively builds an &lt;code&gt;INSERT&lt;/code&gt; query for each element in &lt;code&gt;data&lt;/code&gt; (lines 18-20 highlighted below).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;cartodb&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CartoDBException&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CartoDBAPIKey&lt;/span&gt;

&lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="s"&gt;'spencer.g.boucher@gmail.com'&lt;/span&gt;
&lt;span class="n"&gt;password&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="s"&gt;'redacted'&lt;/span&gt;
&lt;span class="n"&gt;cartodb_domain&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'justmytwospence'&lt;/span&gt;
&lt;span class="n"&gt;API_KEY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'redacted'&lt;/span&gt;
&lt;span class="n"&gt;cl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CartoDBAPIKey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;API_KEY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cartodb_domain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;reading&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;alt&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'alt'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;device&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt;     &lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'device'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;lat&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'lat'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;lon&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'lon'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;os&lt;/span&gt;      &lt;span class="o"&gt;=&lt;/span&gt;     &lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'os'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt;       &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'t'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;version&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;     &lt;span class="n"&gt;reading&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'version'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;query_string&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"INSERT INTO openpaths_justmytwospence (alt, date, device, lat,  lon, os, version, the_geom) "&lt;/span&gt;
                       &lt;span class="s"&gt;"VALUES ({0}, abstime({1}), '{2}', {3}, {4}, '{5}', '{6}', ST_ SetSRID(ST_Point({4}, {3}), 4326))"&lt;/span&gt;  
                      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;cl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query_string&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;CartoDBException&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"some error ocurred"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A few notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It would certainly be faster to insert all of the new data into the
    database using a single &lt;code&gt;INSERT&lt;/code&gt; statement, but that would require
    some more tedious text parsing and execution speed isn’t
    particularly important to us. As it stands, it takes about six
    seconds to post a day’s worth of data.&lt;/li&gt;
&lt;li&gt;One posgreSQL “gotcha” had me hung up for quite some time: single
    quotes parse fine but double quotes do not.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ST_SetSRID&lt;/code&gt; is a &lt;a href="http://postgis.org/docs/ST_SetSRID.html"&gt;PostGIS command&lt;/a&gt; that converts a lon/lat pair
    (in that order - another “gotcha”) to the necessary geometry object.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Last but not least, we need this script to run automatically. Because we’ve written the script to transplant 24 hours of data, we’ll need to run it once a day in order to capture all of the data that’s being generated. I tried to set up my web host, &lt;a href="https://laughingsquid.us/"&gt;LaughingSquid&lt;/a&gt;, to do this, but unfortunately they don’t grant shell access so we can’t install all those fancy python modules that we’ve already used. Its totally possible to rewrite the script to use only modules from the &lt;a href="http://docs.python.org/2/library/"&gt;Python Standard Library&lt;/a&gt;, but this would turn a simple task into a tedious one. Manually implementing OAuth in particular would be a total pain in the rear, and classes are just about to resume after all, so a different solution is in order. Let’s spin up a &lt;a href="http://aws.amazon.com/"&gt;“micro” &lt;span class="caps"&gt;EC2&lt;/span&gt; instance&lt;/a&gt; instead. This gives us free reign to install whatever we need for the low low cost of ¢.02 per hour. This does start to add up, but our Master’s program gives us some pretty substantial Amazon Web Services credit that goes mostly unused, so we aren’t too upset :). &lt;span class="caps"&gt;UPDATE&lt;/span&gt;: A new post provides details about how to schedule Amazon &lt;span class="caps"&gt;EC2&lt;/span&gt; instances - &lt;a href="http://www.spencerboucher.com/ec2-apis/"&gt;http://www.spencerboucher.com/ec2-apis/&lt;/a&gt;.
&lt;br/&gt;
After &lt;code&gt;pip install&lt;/code&gt;ing everything we need and &lt;code&gt;scp&lt;/code&gt;ing our python script (let’s call it update.py) into the home directory of our remote server, all we need to do is set up a crontab with the &lt;code&gt;crontab -e&lt;/code&gt; command and add the following line:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;@daily /usr/bin/python ~/update.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;@daily&lt;/code&gt; is actually a shortcut for &lt;code&gt;* * * * *&lt;/code&gt;, where each asterix is a
placeholder for the (respectively) minute, hour, day of month, month,
and day of week that the script should executed. This shortcut defaults
to midnight every day, which is really as good as anything for our
purposes.
&lt;br/&gt;
Voilà! Now we can step back and relax, knowing that we don’t have to do a single thing and our map will continue to show the most up-to-date data available.
&lt;br/&gt;
A few final notes: 
-   We might reasonably want to lag our script by a week or so, for security/privacy reasons.
-   As far as I can tell, the location readings are recorded in a &lt;a href="http://en.wikipedia.org/wiki/Unix_time"&gt;&lt;span class="caps"&gt;POSIX&lt;/span&gt; time&lt;/a&gt; and have not been adjusted by time zone, so they are still in the &lt;a href="http://en.wikipedia.org/wiki/Coordinated_Universal_Time"&gt;&lt;span class="caps"&gt;UTC&lt;/span&gt;&lt;/a&gt; time zone. This means that they are 8 hours off from the actual time in California, where I usually am. This doesn’t bother me too much at the moment because the visualization is still relatively low resolution in the time domain anyways. At some point I might implement the relevant transformation, but this will raise its own issues because I won’t &lt;em&gt;always&lt;/em&gt; be in California, not to mention all that Daylight Savings nonsense.
-   [Click here for an addendum to this post that will take you through how to schedule the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance and avoid having it run 24/7][]&lt;/p&gt;
&lt;p&gt;[Click here for an addendum to this post that will take you through
  how to schedule the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance and avoid having it run 24/7]: http://www.spencerboucher.com/ec2-apis/
    “Scheduling tasks in the cloud with &lt;span class="caps"&gt;EC2&lt;/span&gt; APIs”&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="api"></category><category term="aws"></category><category term="cartoDB"></category><category term="cartography"></category><category term="cron"></category><category term="ec2"></category><category term="map"></category><category term="quantifiedSelf"></category></entry><entry><title>Luke attempts a novel analysis in SAS</title><link href="http://justmytwospence.github.com/pelican/luke-skywalker-attempts-a-novel-analysis-in-sas.html" rel="alternate"></link><updated>2014-01-06T22:17:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2014-01-06:luke-skywalker-attempts-a-novel-analysis-in-sas.html</id><summary type="html"></summary><category term="SAS"></category><category term="R"></category></entry><entry><title>Luke Skywalker on SAS</title><link href="http://justmytwospence.github.com/pelican/luke-skywalker-on-sas.html" rel="alternate"></link><updated>2013-12-09T00:24:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2013-12-09:luke-skywalker-on-sas.html</id><summary type="html"></summary><category term="SAS"></category><category term="R"></category></entry><entry><title>Kaggle(esque)</title><link href="http://justmytwospence.github.com/pelican/kaggle-esque.html" rel="alternate"></link><updated>2013-12-07T06:38:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2013-12-07:kaggle-esque.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;Our program hosted its own Kaggle-style competition this quarter, featuring a “mystery” dataset with a binary target variable. I was impressed by the reveal.js javascript library, so I put together our final slides using it, both because its pretty, and as a way to get some more practice with &lt;span class="caps"&gt;HTML&lt;/span&gt;, &lt;span class="caps"&gt;CSS&lt;/span&gt;, and javascript. On top of that, I had to expand my knowledge of Git in order to publish it on their GitHub Pages platform, so wins all around. Check out the work of Team LoanShark here:&lt;/p&gt;
&lt;iframe height="500" src="http://justmytwospence.github.io/LoanSharks/slides/#/" width="100%"&gt;
Your browser doesn’t support iframes. Do yourself a favor and go
download a *real* browser
&lt;/iframe&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="javascript"></category><category term="reveal.js"></category></entry><entry><title>Map-time at Stamen</title><link href="http://justmytwospence.github.com/pelican/map-time-at-stamen.html" rel="alternate"></link><updated>2013-12-01T02:37:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2013-12-01:map-time-at-stamen.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;Last week, a classmate and I took a break from coursework to attend one of the many great Meetup events that San Francisco has to offer for data science practitioners. I’ve been pushing myself to attend at least one data-centric Meetup every week, because these events are one of the most amazing parts about going to school in the same place where so many of the biggest names work. To be honest, I believe that becoming a presence in the data science scene and meeting the movers and shakers is equally if not more important than coursework.&lt;/p&gt;
&lt;p&gt;I actually attended 3 meetups last week, one about D3.js at Trulia &lt;span class="caps"&gt;HQ&lt;/span&gt;, one about &lt;span class="caps"&gt;GIS&lt;/span&gt; technologies and the Code for America &lt;span class="caps"&gt;HQ&lt;/span&gt;, and one about mapping at Stamen &lt;span class="caps"&gt;HQ&lt;/span&gt;. I picked all three because they are relevant to a geospatial data visualization that I am working on for my practicum at AutoGrid, but the last one is what I’m going to talk a bit about, because it was the most hands-on.&lt;/p&gt;
&lt;p&gt;The workshop took place at Stamen Design’s headquarters in the Mission and was led by Eric Theise; you can see his beautiful/informative slides (created using &lt;a href="http://lab.hakim.se/reveal-js/#/"&gt;reveal.js&lt;/a&gt;) here:   &lt;a href="http://erictheise.com/maptime_platform_slides/#/"&gt;&lt;/a&gt;&lt;a href="http://erictheise.com/maptime_platform_slides/#/"&gt;http://erictheise.com/maptime_platform_slides/#/&lt;/a&gt;      Some useful Q&amp;amp;A happened on the Meetup event page as well:   &lt;a href="http://www.meetup.com/Maptime-SF/events/147110652/"&gt;&lt;/a&gt;&lt;a href="http://www.meetup.com/Maptime-SF/events/147110652/"&gt;http://www.meetup.com/Maptime-&lt;span class="caps"&gt;SF&lt;/span&gt;/events/147110652/&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;This was actually a two-part workshop, but it was relatively painless to follow the instructions and get up to speed for the part &lt;span class="caps"&gt;II&lt;/span&gt;, so you should really give it a shot even now if it looks interesting.&lt;/p&gt;
&lt;p&gt;First we got postgres up and running on our machines. I have local installations of MySQL, MongoDB, Hadoop and Hive up and running thanks to our course in Distributed Databases, but our class didn’t have time to get to postgres within a single credit hour. This, despite the fact that our professor admits to postgres being the best database to use if you have anything to say about it.&lt;/p&gt;
&lt;p&gt;Next, we populated our database with some data from OpenStreetMap. Mike Migurski extracts data from &lt;span class="caps"&gt;OSM&lt;/span&gt; for major metropolitan areas on a semi-regular basis, so we used the San Francisco data &lt;a href="http://metro.teczno.com/#san-francisco"&gt;available on his web site&lt;/a&gt; via &lt;a href="http://wiki.openstreetmap.org/wiki/Osm2pgsql"&gt;osm2pgsql&lt;/a&gt;, a command-line utility that loads OpenStreetMap data into PostgreSQL databases.&lt;/p&gt;
&lt;p&gt;Then we used &lt;a href="https://www.mapbox.com/tilemill/"&gt;TileMill&lt;/a&gt;, MapBox’s desktop application, to visualize our newborn database. We discovered how remarkably easy it can be to create vector layers for data contained in such a postGIS database using the same old &lt;span class="caps"&gt;SQL&lt;/span&gt; and &lt;span class="caps"&gt;CSS&lt;/span&gt; syntax you already know and love. Eric introduced us to some sensible pre-baked &lt;a href="https://github.com/gravitystorm/openstreetmap-carto"&gt;CartoCSS boilerplate&lt;/a&gt; courtesy of Andy Allen.&lt;/p&gt;
&lt;p&gt;Lastly, we used a nifty feature of TileMill to actually bake our own map tiles and serve them up for use in our own maps. Note that if you want to do this, you’ll need the &lt;a href="https://github.com/mapbox/mbutil"&gt;mbutil command-line utility&lt;/a&gt;, not currently mentioned in the slide deck.&lt;/p&gt;
&lt;p&gt;Not too shabby for 2 hours on a Wednesday night. Many thanks to the guys at Stamen for hosting, especially Eric for all his work on the slides. Not to mention the many other brilliant people who have made the tools and resources that allow something this involved and grandiose to be done on a laptop by someone who is still learning the ropes. Hands-on workshops like this are one of the best ways to learn these technologies. Case in point, I may not have ever stumbled across Mike or Andy’s resources had I not been learning directly from people who are intimately familiar with the practical ins and outs of digital cartography.&lt;/p&gt;
&lt;p&gt;Digital mapping is rapidly capturing my interest because of the beautifully functional things one can do with it, and it seems like an amazing time to be learning it, because the ecosystem is beginning to really flourish. Looking forward to more events!&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="cartography"></category><category term="maps"></category></entry><entry><title>StepLively update</title><link href="http://justmytwospence.github.com/pelican/steplively-update.html" rel="alternate"></link><updated>2013-11-03T21:59:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2013-11-03:steplively-update.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;This week I got an email from RStudio letting me know that after a week or two of waiting, I’ve been assigned &lt;span class="caps"&gt;2GB&lt;/span&gt; of space on their servers for hosting Shiny apps. So now you can click on the above link to see the live version of my stepwise regression app that I posted last week. In celebration, I’ve added a few extras to the app, including the ability to upload your own data set, although its not a particularly robust feature yet.&lt;/p&gt;
&lt;p&gt;Update: I’ve recently moved to Rstudio’s next-gen hosting platform: shinyapps.io. This was necessary because I was having issues installing low-level packages like Rcpp onto their Spark server. This new platform is much more flexible in this regard. I’ve updated links accordingly.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="R"></category><category term="Shiny"></category><category term="stepwise"></category></entry><entry><title>Switching R versions on-the-fly</title><link href="http://justmytwospence.github.com/pelican/switching-r-versions-on-the-fly.html" rel="alternate"></link><updated>2013-10-22T22:01:00+02:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2013-10-22:switching-r-versions-on-the-fly.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;If you need to switch back to an older version of R to use a particular package, it doesn’t get easier than downloading the &lt;a href="http://r.research.att.com/"&gt;RSwitcher utility from &lt;span class="caps"&gt;AT&lt;/span&gt;&amp;amp;T&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I use homebrew to keep my main installation up-to-date, and then switching back to 2.7 when I need a certain package is as easy as one click.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="R"></category></entry><entry><title>Holo color palette</title><link href="http://justmytwospence.github.com/pelican/holo-color-palette.html" rel="alternate"></link><updated>2013-08-20T06:19:00+02:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2013-08-20:holo-color-palette.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;Despite being slightly colorblind, I’m one of those
people who is bizarrely anal-retentive when it comes to color-schemes in
the software that I use. I use custom &lt;span class="caps"&gt;CSS&lt;/span&gt; extensions in Chrome to make
the color schemes of websites like Wikipedia, Youtube, etc a bit more
pleasing. I did some digging in the accessibility preference pane of
Adobe Reader to make the default document background a nice book-like
sepia tone which makes reading PDFs online a much less straining
endeavor (incidentally - Google Play Books &lt;em&gt;only&lt;/em&gt; has a white background
which is why I greatly prefer the Amazon Kindle web app). I’ve spent
wayy too long (as I think many geeks do), customizing my terminal and
custom android &lt;span class="caps"&gt;ROM&lt;/span&gt; themes. I think you get the picture.&lt;/p&gt;
&lt;p&gt;Anyways, I had been using the Solarized palette for a lot of things
because it seems like its choices are a bit less arbitrary and has some
science behind it (although I haven’t done any research into how
legitimate that science is). Its a bit limiting, however, and recently
I’ve been attracted to the Holo color palette that &lt;span class="caps"&gt;ICS&lt;/span&gt; compliant Android
apps use. I couldn’t find a pre-compiled Holo palette in .clr format for
use in the Mac &lt;span class="caps"&gt;OSX&lt;/span&gt; Color Picker utility, so I made a simple, bare-bones
one. First I had to download the &lt;span class="caps"&gt;HEX&lt;/span&gt; plugin from &lt;a href="http://wafflesoftware.net/hexpicker/"&gt;wafflesoftware.net&lt;/a&gt;
which for some strange reason is not a functionality that Apple thought
worthwhile to put into a color picker (???). &lt;a href="http://db.tt/xU7lyIBh"&gt;You can download my .clr
palette right here&lt;/a&gt;. Maybe I can save the next anal-retentive googler
a few minutes.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;PS&lt;/span&gt;. While I am talking about color - if you haven’t heard of &lt;a href="http://justgetflux.com/"&gt;fl.ux&lt;/a&gt;,
check it out. It is a magical little utility that warms the color of
your screen automatically after sunset and can &lt;em&gt;really&lt;/em&gt; save your eyes
and make getting to sleep a little bit easier if you are working in
front of your screen late at night.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="color"></category><category term="holo"></category><category term="solarized"></category></entry><entry><title>Twitter Reaction to Events Often at Odds with Overall Public Opinion</title><link href="http://justmytwospence.github.com/pelican/twitter-reaction-to-events-often-at-odds-with-overall.html" rel="alternate"></link><updated>2013-08-06T00:09:00+02:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2013-08-06:twitter-reaction-to-events-often-at-odds-with-overall.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;[Twitter Reaction to Events Often at Odds with Overall Public Opinion][]&lt;/p&gt;
&lt;p&gt;This is a long-standing beef that I have always had with any Twitter-sentiment analysis. A huge chunk of people tweeting are just whiny adolescents who wouldn’t even be polled if we were to design a huge study. I haven’t heard of any twitter-mining tools or packages that allow you to filter the Twitterers that you are pulling your random sample of tweets from. That would certainly be worthwhile and would make a lot of people care a lot more about Twitter sentiment analyses.&lt;/p&gt;
&lt;p&gt;[Twitter Reaction to Events Often at Odds with Overall Public
  Opinion]: http://www.pewresearch.org/2013/03/04/twitter-reaction-to-events-often-at-odds-with-overall-public-opinion/&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary></entry><entry><title>On being approximate</title><link href="http://justmytwospence.github.com/pelican/on-being-approximate-an-inadvertent-data-poem.html" rel="alternate"></link><updated>2013-07-20T05:39:00+02:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2013-07-20:on-being-approximate-an-inadvertent-data-poem.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;&lt;img alt="An inadvertent data poem" src="http://justmytwospence.github.com/pelican/images/on-being-approximate.png" style="width: 1024px; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;p&gt;An inadvertent data poem by &lt;a href="http://www.usfca.edu/facultydetails.aspx?id=6442485442"&gt;Dr. Cindi Thompson&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="approximation"></category></entry><entry><title>D3 Test</title><link href="http://justmytwospence.github.com/pelican/d3-test.html" rel="alternate"></link><updated>2013-03-21T00:00:00+01:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2013-03-21:d3-test.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;button class="btn btn-info" id="add"&gt;
		Add
	&lt;/button&gt;
&lt;button class="btn btn-info" id="remove"&gt;
		Remove
	&lt;/button&gt;
&lt;div id="svg"&gt;&lt;/div&gt;
&lt;/body&gt;&lt;/html&gt;</summary></entry><entry><title>Science: the art and arborization of knowledge</title><link href="http://justmytwospence.github.com/pelican/science-the-art-and-arborization-of-knowledge.html" rel="alternate"></link><updated>2011-10-06T06:27:00+02:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2011-10-06:science-the-art-and-arborization-of-knowledge.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;I often find it odd that people tend to think of “science” as being somehow classified exclusively from “humanities.” Somehow things like literature, history, and philosophy become the study of the human condition while science becomes simply technobabble ruled by equations and esoteric to the point of being completely inaccesible to anyone that hasn’t spent their life chiseling away at their little block of science.&lt;/p&gt;
&lt;p&gt;But in this era of scientific ultra-specialization, its easy to lose sight of the fact that there is nothing more fundamentally human. Science isn’t a set of equations, the latest &lt;span class="caps"&gt;MRI&lt;/span&gt; scanner, deep-space telescope, or vat of chemicals. In its purest essence, science is a way for us to discover what &lt;em&gt;is.&lt;/em&gt; Actually, it’s the &lt;em&gt;only&lt;/em&gt; way for us to explore truth. There is no other tool available to us- which is why science strikes me as the fundamental study of the human condition and our place in the universe.&lt;/p&gt;
&lt;p&gt;In this respect, science is &lt;em&gt;simple.&lt;/em&gt;Its interesting to note that most of the pivotal, truly &lt;em&gt;insightful&lt;/em&gt;revelations that scientists have dreamed up are so basic a child can understand them immediately. Take, for example, Darwin’s theory of evolution. This is (only somewhat) arguably the most important and unifying principle in all of biology (and perhaps the physical sciences as well, see: &lt;a href="http://www.ted.com/talks/lee_cronin_making_matter_come_alive.html"&gt;Lee Cronin&lt;/a&gt;), but it barely requires an ounce more knowledge than what is plainly available to everyone. Okay, well it did take some island hopping and careful observations, but now that the insight is made, its intuitive. Science is a way of looking at the way things are and gaining both incredible power and profound meaning from it.&lt;/p&gt;
&lt;p&gt;Similarly, while Einstein’s theories of relativity may seem stymying and even intimidating, whats truly amazing is that the conclusions they draw can be (and originally were!) arrived at purely on the basis of thought experiments! Think about that. SImply by changing the way that we &lt;em&gt;conceive&lt;/em&gt; of the things we experience, we can create understanding where there was once nothing.&lt;/p&gt;
&lt;p&gt;Do these kinds of insights still happen in science? Sometimes if feels like there’s no way we will make comparable discoveries in our lifetime. Surely we aren’t still missing something as big and far-reaching as evolution or E=mc\^2… Brain science is one of the few fields of inquiry left in which such a paradigm shift can be made. Its my belief that the seemingly intractable problems of consciousness- of what “we” and our subjective experiences even &lt;em&gt;are-&lt;/em&gt;can and will be enlightened by science. Like evolution and relativity, all it requires is the right way of looking at the issue.&lt;/p&gt;
&lt;p&gt;Of course, its entirely neuro-chauvinistic of me to make such a special claim about the brain. After all, we don’t know what we don’t know, and thats why we have science.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="humanities"></category><category term="paradigm"></category><category term="philosophy"></category><category term="science"></category></entry><entry><title>Modern hierarchy of needs</title><link href="http://justmytwospence.github.com/pelican/one-of-my-major-life-goals-is-to-have-my-own-ted.html" rel="alternate"></link><updated>2011-10-03T03:17:00+02:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2011-10-03:one-of-my-major-life-goals-is-to-have-my-own-ted.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;[gallery]&lt;/p&gt;
&lt;p&gt;One of my major life goals is to have my own &lt;span class="caps"&gt;TED&lt;/span&gt; talk. In fact, I’m pretty sure if Maslow had been doing his research today, the pinnacle of self-actualization atop his hierarchy of needs would look something like this.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="Hierarchy of needs"></category><category term="ideas worth spreading"></category><category term="Maslow"></category><category term="TED"></category></entry><entry><title>Thielfoundation.org</title><link href="http://justmytwospence.github.com/pelican/thielfoundation-org.html" rel="alternate"></link><updated>2011-09-26T03:58:00+02:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2011-09-26:thielfoundation-org.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;&lt;a href="http://www.thielfoundation.org/index.php?option=com_content&amp;amp;id=14:the-thiel-fellowship-20-under-20&amp;amp;catid=1&amp;amp;Itemid=16"&gt;Thielfoundation.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;About a week ago, my house-mate invited me to has to have been one of the most interesting social events I’ve ever lucked into attending. The founder of Paypal, subsequent multimillion(billion?)aire, and venture capitalist Peter Thiel created a foundation that offers fellowships that amount to anti-college scholarships. Under the program name 20 under 20, he provides brilliant, successful, creative teens with 100,000 dollars over the course of two years to &lt;span class="caps"&gt;NOT&lt;/span&gt; go to Ivy League schools and, instead, become entrepreneurs and and develop their ideas. They must have had some great applications because in their first year, the foundation couldn’t settle on 20 and so there’s something like 23 or 24 in actuality. &lt;/p&gt;
&lt;p&gt;Incredible premise, you might say. And incredible it is.&lt;/p&gt;
&lt;p&gt;My aforementioned house-mate (I haven’t decided if I am going to use real names in this blog yet) has somehow fallen in a few of the fellows, and let me tag along to a social get-together of the foundation. It took place right on the Bay in the Berkeley marina, surrounded by towering sailboats docked on shore.&lt;/p&gt;
&lt;p&gt;We got there fairly late, but during the time that we were there we sure met some interesting characters. My housemate introduced me to a leader in the “seasteading” movement- which is an effort to develop offshore sovereign communities outside of any existing federal jurisdiction in an effort to establish and test alternative, highly libertarian societal structures. Or something like that. In addition, I was brought up to speed on the status of the first “charter cities” being created in Honduras. There is a fantastic &lt;span class="caps"&gt;TED&lt;/span&gt; talk concerning these charter cities that I highly recommend. &lt;/p&gt;
&lt;p&gt;That conversation led us into a discussion with a gentleman about the possibilities such a city would present for medical tourism. Eventually it was discovered that he works for a cryogenics biotech company, and actually used to be the president of Alcor Life Extension Foundation. I’ve been interested in Alcor for a while because its such a controversial and fascinating idea- they will chemically and cryogenically preserve your body (or just your head- if you are on a budget) after death so that if and when we develop technologies to reinstate neural activity, you’ll still be there to do so. This of course raises a whole host of interesting philosophical issues, like what kind of role continuity of neural activity plays in the issue of consciousness or identity, if any. Maybe another blog post on that some day soon.&lt;/p&gt;
&lt;p&gt;Later I met someone working on developing models of brain function in the human neocortex. One of their key premises is that we can create these models in any way we want as long as we successfully mimic the output of the brain, but I happen to disagree on this point. Modeling true intelligence is more than just mimicking output, the &lt;em&gt;way&lt;/em&gt; in which computation is performed is nontrivial. Apparently, another company in the Bay area is taking exactly this approach. Numenta was founded by Jeff Hawkins, the guy who invented Palm Pilot, and is developing cortical learning algorithms that are based on actual biology- much more meaningful in my opinion. I’m in the process of reading everything about Numenta’s work at the moment, and I highly recommend Jeff Hawkins’ &lt;span class="caps"&gt;TED&lt;/span&gt; talk as well if you are at all interested in &lt;span class="caps"&gt;AI&lt;/span&gt;, the brain, or intelligence. There will definitely be another blog post in the near future concerning this.&lt;/p&gt;
&lt;p&gt;Oh, and we almost got abducted by a taco truck. We were both starving and heard the food truck was about to leave, so we ran out to it, but it looked closed up. Several people were climbing in the side door however, so we followed them in hunger-fueled desperation, only to find the door closed behind us. Apparently these other people just needed a ride back to San Fran and were hitchhiking on the truck, but fortunately we managed to bail out before we wound up as unwitting taco-making indentured servants.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><category term="entrepeneurship"></category><category term="silicon valley"></category></entry><entry><title>Nonlinear</title><link href="http://justmytwospence.github.com/pelican/nonlinear.html" rel="alternate"></link><updated>2011-09-25T00:59:00+02:00</updated><author><name>Spencer Boucher</name></author><id>tag:justmytwospence.github.com/pelican,2011-09-25:nonlinear.html</id><summary type="html">&lt;html&gt;&lt;body&gt;&lt;p&gt;If there is one thing I have learned since graduating from college, it is that life is not linear. I feel that high school, college, and social constructions I have grown up in had programmed me to conceive of life in a very stepwise, linear way. Maybe it’s because you are working so single-mindedly toward holding that diploma in your hand. Or perhaps because you just don’t have much exposure beyond the people who are doing mostly what you are doing. I could be wrong, but I think a lot of college kids have this mentality.&lt;/p&gt;
&lt;p&gt;But it just isn’t. Or at least it doesn’t have to be if you don’t want it to be. You can do absolutely anything you want to in this world, and the arena is &lt;span class="caps"&gt;HUGE&lt;/span&gt;. Like, unimaginably huge. And the Bay Area has to be one of the most incredible places in the world to make this fact hit home. My new home is a mecca for so many different cultural movements. Silicon Valley, entrepeneurs, artists, hippies, &lt;span class="caps"&gt;LGBT&lt;/span&gt; communities, futurists, libertarians, and of course scientists, the last of which I’d like to think I fall into.&lt;/p&gt;
&lt;p&gt;My favorite thing about moving here by far is simply how &lt;em&gt;interesting&lt;/em&gt; everyone I meet is. I mean really really fascinating. Maybe I’ve just been somehow missing how intrinsically awesome the random people I would meet in Tennessee and Houston were. But here, in a single night you can meet people who work for Google and Facebook, people who hunt planets for &lt;span class="caps"&gt;NASA&lt;/span&gt;, people crafting all manner of crazy or thoughtful or brilliant startups, people who have attended universities all over the country and the world… I could go on and on. &lt;/p&gt;
&lt;p&gt;That’s one thing I hope to do with this blog: highlight the amazing people and projects that I come across. These people are not living linear lives. And just being in this crazy, fast paced, marvelously jumbled community is creating a paradigm shift in the way that I see the world. Its tough to explain, but I guess I’m realizing how things are out there for you to take and affect and become a part of- &lt;em&gt;big&lt;/em&gt; things- and all you have to do is try. The rough plan for this blog is to make it about &lt;strong&gt;ideas&lt;/strong&gt;, &lt;strong&gt;adventures&lt;/strong&gt;, and &lt;strong&gt;discoveries&lt;/strong&gt;. No promises for how it will turn out developing though.&lt;/p&gt;
&lt;p&gt;Here’s to being nonlinear!&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary></entry></feed>