<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>justmytwospence</title><link href="http://justmytwospence.github.com/pelican/" rel="alternate"></link><link href="http://justmytwospence.github.com/pelican/feeds/nix-design-python-web.atom.xml" rel="self"></link><id>http://justmytwospence.github.com/pelican/</id><updated>2014-01-08T08:50:00+01:00</updated><entry><title>Live mapping</title><link href="http://justmytwospence.github.com/pelican/live-mapping.html" rel="alternate"></link><updated>2014-01-08T08:50:00+01:00</updated><author><name>justmytwospence</name></author><id>tag:justmytwospence.github.com/pelican,2014-01-08:live-mapping.html</id><summary type="html">&lt;p&gt;[dropcap]I[/dropcap]&amp;#8217;ve been wanting to do some more mapping stuff since
my first encounter with Leaflet a month or two ago while I was working
on a project for [AutoGrid][]. I had my eye on CartoDB&amp;#8217;s time series
library, [Torque][], because I had really wanted to do some time-series
visualization, but time constraints and privacy issues with uploading
data to CartoDB&amp;#8217;s servers prevented me from really exploring. Since I
had a few days of free time over winter break, I played around with it a
bit and came up with this: [spencerboucher.com/map][]. How&amp;#8217;d I do&amp;nbsp;it?&lt;/p&gt;
&lt;p&gt;First I needed some geographic data, so I turned to a source of data
I&amp;#8217;ve been collected for almost a year - my own location. [OpenPaths][]
is a mobile app that records your location at regular time intervals. I
opted for every 30 minutes at first, then upped it to every 15 minutes
when I discovered that the effect on battery life wasn&amp;#8217;t nearly as bad
as I expected it to be. OpenPaths is a project of [the R&amp;amp;D department at
The New York Times][] and they [claim][] that you are the only one with
access to the collected data. Interestingly, you can grant various
[research programs][] access to your data at your own discretion. Your
data is conveniently downloadable as a csv, json, or kml file, so I
easily pulled my dataset of \~3,000 time points since December 2012.
Unfortunately, I made the switch from iPhone to Android around April
(well, that part is fortunate), and forgot to re-download the app, so I
only really have data from the around the first three months and last
two months of&amp;nbsp;2013.&lt;/p&gt;
&lt;p&gt;
Turns out, making impressive maps with CartoDB is almost embarrassingly
easy. Their &lt;span class="caps"&gt;GUI&lt;/span&gt; is pretty intuitive and running queries on their
postgreSQL database is simple. Even time series stuff built on the
Torque backend is really just point and click. I decided that the best
way to visualize this data was with an aggregated hexbin heatmap of all
my past locations, overlaid with a point-by-point replay with a
time-slider. From there, it was just a one-line &lt;span class="caps"&gt;API&lt;/span&gt; call to host the map
on my website (line 30 highlighted below), which is significantly easier
than the legwork that went into crafting a Leaflet map &amp;#8220;manually.&amp;#8221;

wzxhzdk:0


[hr][dropcap]T[/dropcap]his is pretty awesome, but in light of how easy
it all was, I was almost disappointed. Can we take it one step further?
Let&amp;#8217;s put on our [Quantified Self][] hats and set about to make this map
*live*. There&amp;#8217;s three components to making this happen, so we&amp;#8217;ll step
through them one at a time. First we need to access the most recent data
from OpenPaths (there&amp;#8217;s an &lt;span class="caps"&gt;API&lt;/span&gt; for that!), and then we need to insert
that data into CartoDB&amp;#8217;s database (guess what, there&amp;#8217;s an &lt;span class="caps"&gt;API&lt;/span&gt; for that
too). Last but not least, we need to schedule that data transplant to
occur on a regular basis. The Unix utility `cron` is the canonical tool
for this type of thing, so this seemed like a good time to learn how to
use it.

Python has a reputation for being a great &amp;#8220;glue&amp;#8221; language, so that&amp;#8217;s
what we&amp;#8217;ll use to build this
script.[hr][dropcap]1[/dropcap]Programmatically accessing your data from
OpenPaths is super simple. This piece of our script is pulled more or
less verbatim from [the OpenPaths &lt;span class="caps"&gt;API&lt;/span&gt; documentation][]. Line 21
(highlighted below) is key - this is where we specify which data you
want to pull for injection into the CartoDB database. Here we will grab
the last 24 hours of data (\~96 readings, if you&amp;#8217;re collecting every 15
minutes like me), getting the results in a nice &lt;span class="caps"&gt;JSON&lt;/span&gt;-formatted variable
named `data`.

~~~~ {.lang:python .mark:21 .decode:true title=&amp;#8221;OpenPaths &lt;span class="caps"&gt;API&lt;/span&gt;&amp;#8221;}
import oauth2, time, urllib, urllib2, json

&lt;span class="caps"&gt;ACCESS&lt;/span&gt; = &amp;#8216;redacted&amp;#8217;
&lt;span class="caps"&gt;SECRET&lt;/span&gt; = &amp;#8216;redacted&amp;#8217;
&lt;span class="caps"&gt;URL&lt;/span&gt; = &amp;#8216;https://openpaths.cc/api/1&amp;#8217;

def build_auth_header(url, method):
    params = {                                            
        &amp;#8216;oauth_version&amp;#8217;: &amp;#8220;1.0&amp;#8221;,
        &amp;#8216;oauth_nonce&amp;#8217;: oauth2.generate_nonce(),
        &amp;#8216;oauth_timestamp&amp;#8217;: int(time.time()),
    }
    consumer = oauth2.Consumer(key=&lt;span class="caps"&gt;ACCESS&lt;/span&gt;, secret=&lt;span class="caps"&gt;SECRET&lt;/span&gt;)
    params[&amp;#8216;oauth_consumer_key&amp;#8217;] = consumer.key 
    request = oauth2.Request(method=method, url=url, parameters=params)    
    signature_method = oauth2.SignatureMethod_HMAC_SHA1()
    request.sign_request(signature_method, consumer, None)
    return request.to_header()

now = time.time()
params = {&amp;#8216;start_time&amp;#8217;: now - 24*60*60, &amp;#8216;end_time&amp;#8217;: now} # get the last 24 hours
query = &amp;#8220;%s?%s&amp;#8221; % (&lt;span class="caps"&gt;URL&lt;/span&gt;, urllib.urlencode(params))
#print(query)
try:
    request = urllib2.Request(query)
    request.headers = build_auth_header(&lt;span class="caps"&gt;URL&lt;/span&gt;, &amp;#8216;&lt;span class="caps"&gt;GET&lt;/span&gt;&amp;#8217;)
    connection = urllib2.urlopen(request)
    data = json.loads(&amp;#8221;.join(connection.readlines()))
    print(json.dumps(data, indent=4))
except urllib2.HTTPError as e:
    print(e.read())

wzxhzdk:1


A few notes:

-   It would certainly be faster to insert all of the new data into the
    database using a single `&lt;span class="caps"&gt;INSERT&lt;/span&gt;` statement, but that would require
    some more tedious text parsing and execution speed isn&amp;#8217;t
    particularly important to us. As it stands, it takes about six
    seconds to post a day&amp;#8217;s worth of data.
-   One posgreSQL &amp;#8220;gotcha&amp;#8221; had me hung up for quite some time: single
    quotes parse fine but double quotes do not.
-   `ST_SetSRID` is a [PostGIS command][] that converts a lon/lat pair
    (in that order - another &amp;#8220;gotcha&amp;#8221;) to the necessary geometry object.

[hr]

[dropcap]3[/dropcap]Last but not least, we need this script to run
automatically. Because we&amp;#8217;ve written the script to transplant 24 hours
of data, we&amp;#8217;ll need to run it once a day in order to capture all of the
data that&amp;#8217;s being generated. I tried to set up my web host,
[LaughingSquid][], to do this, but unfortunately they don&amp;#8217;t grant shell
access so we can&amp;#8217;t install all those fancy python modules that we&amp;#8217;ve
already used. Its totally possible to rewrite the script to use only
modules from the [Python Standard Library][], but this would turn a
simple task into a tedious one. Manually implementing OAuth in
particular would be a total pain in the rear, and classes are just about
to resume after all, so a different solution is in order. Let&amp;#8217;s spin up
a [&amp;#8220;micro&amp;#8221; &lt;span class="caps"&gt;EC2&lt;/span&gt; instance][] instead. This gives us free reign to install
whatever we need for the low low cost of ¢.02 per hour. This does start
to add up, but our Master&amp;#8217;s program gives us some pretty substantial
Amazon Web Services credit that goes mostly unused, so we aren&amp;#8217;t too
upset :). &lt;span class="caps"&gt;UPDATE&lt;/span&gt;: A new post provides details about how to schedule
Amazon &lt;span class="caps"&gt;EC2&lt;/span&gt; instances - &lt;http://www.spencerboucher.com/ec2-apis/&gt;.

After `pip install`ing everything we need and `scp`ing our python script
(let&amp;#8217;s call it update.py) into the home directory of our remote server,
all we need to do is set up a crontab with the `crontab -e` command and
add the following line:


wzxhzdk:2


`@daily` is actually a shortcut for `* * * * *`, where each asterix is a
placeholder for the (respectively) minute, hour, day of month, month,
and day of week that the script should executed. This shortcut defaults
to midnight every day, which is really as good as anything for our
purposes.

[hr]

Voilà! Now we can step back and relax, knowing that we don&amp;#8217;t have to do
a single thing and our map will continue to show the most up-to-date
data available. A few final notes:

-   We might reasonably want to lag our script by a week or so, for
    security/privacy reasons.
-   As far as I can tell, the location readings are recorded in a [&lt;span class="caps"&gt;POSIX&lt;/span&gt;
    time][] and have not been adjusted by time zone, so they are still
    in the [&lt;span class="caps"&gt;UTC&lt;/span&gt;][] time zone. This means that they are 8 hours off from
    the actual time in California, where I usually am. This doesn&amp;#8217;t
    bother me too much at the moment because the visualization is still
    relatively low resolution in the time domain anyways. At some point
    I might implement the relevant transformation, but this will raise
    its own issues because I won&amp;#8217;t *always* be in California, not to
    mention all that Daylight Savings nonsense.
-   [Click here for an addendum to this post that will take you through
    how to schedule the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance and avoid having it run 24/7][]

  [AutoGrid]: http://auto-grid.com
  [Torque]: https://github.com/cartodb/torque
  [spencerboucher.com/map]: http://www.spencerboucher.com/map
  [OpenPaths]: http://openpaths.cc
  [the R&amp;D department at The New York Times]: http://nytlabs.com/
  [claim]: https://openpaths.cc/&lt;span class="caps"&gt;FAQ&lt;/span&gt;
  [research programs]: https://openpaths.cc/projects
  [Quantified Self]: http://quantifiedself.com/about/
  [the OpenPaths &lt;span class="caps"&gt;API&lt;/span&gt; documentation]: https://openpaths.cc/api
  [CartoDB&amp;#8217;s &lt;span class="caps"&gt;SQL&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; documentation]: http://developers.cartodb.com/documentation/sql-api.html
  [python module]: https://github.com/vizzuality/cartodb-python
  [PostGIS command]: http://postgis.org/docs/ST_SetSRID.html
  [LaughingSquid]: https://laughingsquid.us/
  [Python Standard Library]: http://docs.python.org/2/library/
  [&amp;#8220;micro&amp;#8221; &lt;span class="caps"&gt;EC2&lt;/span&gt; instance]: http://aws.amazon.com/
  [&lt;span class="caps"&gt;POSIX&lt;/span&gt; time]: http://en.wikipedia.org/wiki/Unix_time
  [&lt;span class="caps"&gt;UTC&lt;/span&gt;]: http://en.wikipedia.org/wiki/Coordinated_Universal_Time
  [Click here for an addendum to this post that will take you through
  how to schedule the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance and avoid having it run 24/7]: http://www.spencerboucher.com/ec2-apis/
    &amp;#8220;Scheduling tasks in the cloud with &lt;span class="caps"&gt;EC2&lt;/span&gt;&amp;nbsp;APIs&amp;#8221;</summary><category term="api"></category><category term="aws"></category><category term="cartoDB"></category><category term="cartography"></category><category term="cron"></category><category term="ec2"></category><category term="map"></category><category term="quantifiedSelf"></category></entry></feed>